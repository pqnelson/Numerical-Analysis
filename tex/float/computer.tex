We can now refine our model of floating-point arithmetic to factor in
the aspects we simplified:
\begin{enumerate}
\item Exponents range from $e_{\text{min}}$ to $e_{\text{max}}$ (finite integers)
\item Infinities are encoded as numbers (denoted $+\infty$ and $-\infty$)
\item Invalid operations are encoded as special quantities, denoted
  ``\NaN'', called ``Not-a-Number''
\end{enumerate}
Everything that has been said so far still holds for actual \ieee-754
floating-point numbers (non-associativity, the fundamental axiom of
floating-point, rounding schemes, etc.). But the algorithm and proof may
no longer be correct when underflowing occurs.

Further, the arithmetic operations may be given a more concrete
implementation, and we have grounds for preferring a radix $\beta=2$.

\subsection{Extended Real Number System*}\label{subsec:computer:extended-real-number-system}

We should bear in mind that \ieee-754 floating-point arithmetic strives
to emulate the \emph{extended reals}
\begin{equation}
  \extendedRR = \RR\cup\{-\infty,+\infty\}.
\end{equation}
The usual intuition holds with arithmetic operators involving
infinities, except the following are ``indeterminate forms'' and illegal
(left undefined):
\begin{itemize}
\item $\infty-\infty$
\item $0\times(\pm\infty)$
\item $\pm\infty/\infty$ (and $\pm\infty/(-\infty)$).
\end{itemize}
Though it is useful in probability and measure theory to define
$0\times(\pm\infty)=0$.

\begin{axiom}[Addition]
  For any finite $x\in\RR$, we have
  $$x+\infty=\infty+x=\infty.$$
  Further
  $$\infty+\infty=\infty.$$
\end{axiom}


\begin{xca}
  Prove or find a counter-example: addition is commutative in the
  extended real-number system.
\end{xca}


\begin{axiom}[Subtraction]
  For any finite $x\in\RR$, we have
  $$x-\infty=-\infty+x=-\infty.$$
  Further, we have
  $$-\infty-\infty=-\infty.$$
\end{axiom}


\begin{axiom}[Multiplication]
  For any positive $x\in\RR$, $x>0$, we have
  $$x\times(\pm\infty)=(\pm\infty)\times x = \pm\infty$$
  and
  $$(-x)\times(\pm\infty)=(\pm\infty)\times(-x)=\mp\infty.$$
  This permits us to treat $-\infty$ as $-1\times(+\infty)$.
\end{axiom}


\begin{xca}
  Prove or find a counter-example: multiplication is commutative in the
  extended real-number system.
\end{xca}


\begin{axiom}[Division]
  If $x\in\RR$, then
  $$\frac{x}{\infty}=\frac{x}{-\infty}=0$$.
\end{axiom}

\begin{rmk}[Extended Reals with Dedekind Cuts]
For real analysts (and lovers of set theoretic constructs), we could
define $\extendedRR$ using Dedekind cuts. Recall, a Dedekind cut is a
pair $(L,U)$ of subsets $L\subset\QQ$ and $U\subset\QQ$ such that
\begin{itemize}
\item $L\neq\emptyset$ and $U\neq\emptyset$
\item if $a\in L$ and $b\in U$, then $a<b$
\item if $a\in L$, then there is an $\ell\in L$ such that $a < \ell$
\item if $b\in U$, then there is some $u\in U$ such that $u < b$
\item if $a\in L$ and $b\in\QQ$ such that $b < a$, then $b\in L$
\item if $b\in U$ and $u\in\QQ$ such that $b < u$, then $u\in U$
\end{itemize}
Then a real number is defined as a cut $x := (L,U)$ such that every
$\ell\in L$ satisfies $\ell < x$, and every $u\in U$ satisfies $x < u$.

We can construct the extended reals $\extendedRR$ using ``extended
Dedekind cuts'', permitting (1) $L=\emptyset$ and $U=\QQ$, and (2)
$L=\QQ$ and $U=\emptyset$. These model $-\infty$ and $+\infty$,
respectively. All other conditions on Dedekind cuts (besides
non-emptiness) are imposed on extended Dedekind cuts, and we get a
rigorous construction of the extended real number system.
\end{rmk}


\begin{xca}
  Prove or find a counter-example: $\extendedRR$ is an ordered field, an
  ordered ring, or a group (using either multiplication or addition, whichever).
\end{xca}


\begin{xca}
  If we consider $\extendedRR^{*}=\extendedRR\cup\{u\}$ where $u$ is the
  special ``undefined'' constant, where we now have $\infty-\infty=u$,
  $0\times(\pm\infty)=u$, $(\pm\infty)/\infty=(\pm\infty)/(-\infty)=u$,
  and for any $a\in\extendedRR^{*}$ and any binary operation
  $\star\in\{+,-,\times,/\}$ we have $u\star a = a\star u = u$;
  then does $\extendedRR^{*}$ form a field? A ring? A group?
\end{xca}


\begin{xca}
  When working in $\extendedRR$, what should $\tanh(\pm\infty)$ be?
  [Hint: consider what $\exp(\pm\infty)$ should be, then use the
    definition of hyperbolic tangent.]
\end{xca}


\begin{xca}
  In the extended reals, what should $\arctan(\pm\infty)$ be?
\end{xca}

\subsection{Defining Numbers}
\begin{defn}
  Let $\beta>1$ be a fixed integer, $t>0$ an integer,
  $e_{\text{min}}<-t<0<t<e_{\text{max}}$ be
  fixed integers. Then we define the set of \define{Floating-Point Numbers}
  in radix $\beta$ to be the set
  \begin{equation}
    \mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}} = \{(s,m,e)\in\ZZ^{3}|
    s\in\{0,1\}, 0\leq m<\beta^{t}, e_{\text{min}}\leq e\leq e_{\text{max}}\}.
  \end{equation}
  We call $m$ the \define{Mantissa} (or \emph{Significand}), $s$ the
  \define{Sign}, and $e$ the \define{Exponent}.
\end{defn}


\begin{danger}
  In practice, it's not uncommon to interpret the mantissa $m$ as
  $f=m/\beta^{t}$ a fraction between $1/\beta\leq f<1$. The \ieee-754
  standard treats floating-point numbers in this manner, as does
  Knuth~\cite{taocp2}.
\end{danger}


\begin{ddanger}
  We should observe that any number $x\in\RR$ such that $|x|<\beta^{e_{\text{min}}}$
  or $|x|>\beta^{e_{\text{max}}}$ cannot be represented in
  the floating-point system. What happens for
  $|x|>\beta^{e_{\text{max}}}$, such numbers are rounded to $\pm\infty$
  (with the same sign as $x$). For tiny numbers $|x|<\beta^{e_{\text{min}}}$,
  we either use subnormal numbers $x=\pm m\times\beta^{e_{\text{min}}-t}$
  or for even smaller numbers\dots we just round $x$ down to zero.
\end{ddanger}


\begin{rmk}
  The \ieee-754 standard specifies the following bounds for $\beta=2$:
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
      Name   & $e_{\text{min}}$ & $e_{\text{max}}$ & $t$ \\\hline
      Single & $-126$         & $127$          & $24$\\
      Double & $-1022$        & $1023$         & $53$\\
      Quad   & $-16382$       & $16383$        & $113$\\
      Octuple & $-262142$     & $262143$       & $237$
    \end{tabular}
  \end{center}
  Most contemporary hardware (as of 2021) supports single and
  double-precision floating-point, and have begun to support
  quad-precision arithmetic (e.g., Intel i5-4440S apparently supports
  it, or more precisely: \textsc{gfortran} $9.3.0$ supports
  it). \emph{Double-precision should be the default} (unless speed is important
  and precision is less important, in which single-precision may be used).
\end{rmk}


\begin{defn}
  We call a number $r\in\RR$ \define{Representable} in
  $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$ if we can write
  \begin{equation}
    r = (-1)^{s}\frac{m}{\beta^{t}}\beta^{e}
  \end{equation}
  for some $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$.
\end{defn}


\begin{defn}
  Floating-point numbers with $\beta=2$ and $e>e_{\text{min}}$, we may
  assume the mantissa $m$ has implicitly a leading-bit of 1,
  called the \define{Implicit Leading Bit}.
\end{defn}


\begin{rmk}
  \ieee-754 specifies $\beta=2$ or $\beta=10$. Numerous studies~\cite{DBLP:journals/corr/abs-1004-3374,10.1145/363235.363240,5009112,10.1145/362003.362013,4039164}
have found $\beta=2$ with an implicit leading bit has better worst-case
and average-case accuracy than other choices of $\beta$. Although, there
is a theoretical case to be made for $\beta=\E\approx2.71828\dots$ as a
choice of radix, its implementation seems impossible.
\end{rmk}


\begin{defn}\label{def:computer-float:normal-subnormal-denormal}
We call a representable floating-point number $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
\begin{itemize}
\item\define{Normal} if $\beta^{t-1}\leq m<\beta^{t}$ or $m=e=0$;
\item\define{Subnormal} if $e=e_{\text{min}}$ and $0<m<\beta^{t-1}$;
\item All others are \define{Denormalized}.
\end{itemize}
\end{defn}


\begin{thm}
  Every representable number in $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
  is either normal or subnormal, but not both.
\end{thm}
\begin{proof}
  Let  $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$ be
  arbitrary. By
  definition~\ref{def:computer-float:normal-subnormal-denormal},
  it's either normal, subnormal, or denormal. We need to consider the
  denormal situation.

  \textsc{Case 1:} If $m=0$, then we have two subcases.

  {}\quad\textsc{Subcase 1:} If $e=0$, then we have $(0,0,0)=0$ which is
  normal.

  {}\quad\textsc{Subcase 2:} If $e\neq0$, then $(0,0,e)=\fl(\beta^{e})$
  which can be encoded as $(0,\beta^{t-1}, e-(t-1))$ which is normal for
  $e_{\text{min}}<e-(t-1)<e_{\text{max}}$ and rounds to zero or infinity
  otherwise.
  
  \textsc{Case 2:} If $m\neq0$, then $m<\beta^{t-1}$, so it's ``leading
  digit'' is 0. If we write the base-$\beta$ expansion
  \begin{equation}
    m=\sum^{t-1}_{i=0}d_{i}\beta^{i}
  \end{equation}
  and suppose $d_{t-1}=\dots=d_{t-k}=0$ but $d_{t-(k+1)}\neq0$ (for some
  $k\geq1$), then $(s,m\beta^{\ell},e-\ell)$ where $\ell=\min\{k,e-e_{\text{min}}\}$
  is either normal (when $\ell=k$) or subnormal (when $\ell<k$).
\end{proof}

\begin{defn}
Let $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
have $0<m<\beta^{t-1}$. Then we define the transformation
\begin{equation}
  (s,m,e)\to(s,m\beta^{\ell},e-\ell)
\end{equation}
where $\ell$ is the number of leading zeroes of the mantissa, the
transformation is defined as \define{Normalization}.
\end{defn}

\begin{rmk}
In other words, any denormalized floating-point number can be normalized
into a subnormal or normal.
\end{rmk}


\begin{thm}
The normal and subnormal floating-point representations are unique.
\end{thm}
\begin{proof}
\textsc{Case 1:} for subnormal floating-point numbers, they are clearly
unique since it amounts to a choice of sign and mantissa.

\textsc{Case 2:} for normal floating-point numbers, suppose
$(s_{1},m_{1},e_{1})$ and $(s_{2},m_{2},e_{2})$ encode the same number
\begin{equation}
  x = (-1)^{s_{1}}(m_{1}/\beta^{t})\beta^{e_{1}} = (-1)^{s_{2}}(m_{2}/\beta^{t})\beta^{e_{2}}.
\end{equation}
Well, necessarily $s_{1}=s_{2}$ and $\beta^{t-1}\leq m_{j} < \beta^{t}$.
We see then that
\begin{equation}
  m_{1}\beta^{e_{1}-t} = m_{2}\beta^{e_{2}-t}
\end{equation}
imply
\begin{equation}
  m_{1} = m_{2}\beta^{e_{2}-e_{1}}.
\end{equation}
But for the bounds to be satisfied, $e_{2}=e_{1}$, which then implies
$m_{1}=m_{2}$.

Hence any $x\in\RR$ representable as a floating-point number, has a unique
representation $y\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$.
\end{proof}


\begin{defn}
  The largest finite floating-point number in $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
  is denoted
  \begin{equation}
    \Omega := (\beta - \beta^{1-t})\beta^{e_{\text{max}}}.
  \end{equation}
  The smallest positive subnormal number is denoted
  \begin{equation}
    \alpha := \beta^{e_{\text{min}}-t+1}. 
  \end{equation}
\end{defn}

\subsubsection{Bounds on Precision and Accuracy}

\begin{defn}
  In the floating-point system $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
  with rounding-scheme $R\colon\RR\to\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$,
  we define the \define{Machine Precision} as the real number
  $\machinePrec\in\RR$
  which is the bound on rounding-error.
\end{defn}


\begin{ex}
For truncation $RZ(-)$, the machine precision is
\begin{equation}
  \machinePrecision = \beta^{1-t}
\end{equation}
by Lemma~\ref{lemma:idealized:truncation-round-off-error}.
\end{ex}


\begin{ex}
For round-to-nearest $RN$, the machine precision is
\begin{equation}
  \machinePrecision = \frac{\beta^{1-t}}{2}
\end{equation}
by Lemma~\ref{lemma:idealized:rn-round-off-error}. Since this is the
default rounding scheme for \ieee-754, this should be the quantity we
remember. We also observe this corresponds with our
Definition~\ref{defn:idealized:machine-epsilon} of machine epsilon,
carried over to \ieee-754 floating-point.
\end{ex}


\begin{defn}[{{\it Handbook\/}~\cite[(Def.2.6)]{10.5555/3235984}}]
  If $x\in\RR$ is representable by a finite floating-point number
  $u=\fl(x)$ such that $|u|\neq\Omega$ (so there is a next consecutive
  floating-point number and a preceding floating-point number),
  if $\beta^{n}\leq |u| < \beta^{n+1}$
  we define the \define{Unit in Last Place} of $x$ to be the real-number
  $\ulp(x) = \beta^{n-(t-1)}$.
\end{defn}

\begin{rmk}
I should have some discussion about the definition of $\ulp(-)$, which
is another muddled concept. See, e.g., Muller~\cite{muller2005ulp} for a
discussion of the various definitions. Knuth~\cite[pp.232--233]{taocp2}
defines $\beta^{1-t}$ as an ulp, but this seems to be antiquated terminology.
Harrison~\cite{harrison-hol99} seems to be the first to formalize the
notion in a theorem prover.
\end{rmk}

\begin{defn}
  If $u=\fl(x)$, we define $\ULP(u)=\ulp(x)$.
\end{defn}


\begin{ex}
  This $\ulp(-)$ function depends on the rounding scheme.
  We see that $\ulp(\RN(1)) = 2\machinePrec$ and $\ulp(\RZ(1))=\machinePrec$.
\end{ex}


\begin{thm}
  If $\fl(x)=(s,m,e)$, then $\ulp(x) = \beta^{\max(e,e_{\text{min}})-(t-1)}$.
\end{thm}


\begin{thm}
  In base $\beta=2$, we have for any $x\in\RR$ and float $u$,
  \begin{equation*}
    |\fl^{-1}(u)-x| < \frac{1}{2}\ulp(x)\implies u = \fl(x).
  \end{equation*}
\end{thm}


\begin{thm}
  For any real number $x\in\RR$ representable by a finite floating-point
  number $u:=\fl(x)$, we have
  \begin{equation*}
    |\fl^{-1}(u)-x| < \frac{1}{2}\ulp(x).
  \end{equation*}
\end{thm}


\begin{thm}
  For any real number $x\in\RR$ representable by a finite floating-point
  number $u:=\fl(x)$, we have
  \begin{equation*}
    |\fl^{-1}(u)-x| < \frac{1}{2}\ULP(u).
  \end{equation*}
\end{thm}


\begin{thm}
  For any real number $x\in\RR$ representable by a finite floating-point
  number $u=\RD(x)$ or $u=\RU(x)$, we have
  \begin{equation*}
    |\fl^{-1}(u)-x|\leq\ulp(x).
  \end{equation*}
\end{thm}


\begin{thm}
  For any real number $x\in\RR$ representable by a finite floating-point
  number $u=\RD(x)$ or $u=\RU(x)$, we have
  \begin{equation*}
    |\fl^{-1}(u)-x|\leq\ULP(u).
  \end{equation*}
\end{thm}

\subsubsection{Representing Infinities}

In $\beta=2$, an infinity is represented by $(s,0,e_{\text{max}})=(-1)^{s}\infty$.
Floating-point infinities enjoy all the properties we may expect, e.g.,
$\infty + x = x + \infty = \infty$ for any finite floating-point $x$.

\begin{axiom}
  We have
  \begin{equation*}
    (-\infty)\otimes(-\infty) = \infty\otimes\infty = +\infty.
  \end{equation*}
\end{axiom}

\begin{axiom}
  We have $\infty\ominus(-\infty)=\infty\oplus\infty=\infty$.
\end{axiom}

\begin{axiom}
  We have $(-\infty)\oplus(-\infty)=(-\infty)\ominus(+\infty)=-\infty$.
\end{axiom}

\begin{axiom}
  We have $\sqrt{+\infty}=+\infty$.
\end{axiom}

\begin{axiom}
  If $u>0$ is a finite floating-point number, then
  \begin{equation*}
    \frac{\pm u}{0} = \pm\infty.
  \end{equation*}
\end{axiom}

\begin{axiom}
  If $u$ is a finite floating-point number, then
  \begin{equation*}
    \frac{u}{\pm\infty}=0.
  \end{equation*}
\end{axiom}

\begin{axiom}
In floating-point arithmetic, we have $\log(0)=-\infty$ and $\log(+\infty)=+\infty$.
\end{axiom}

\subsubsection{Encoding NaN}

The \ieee-754 standard attempts to emulate \emph{real} numbers. There
are times when we could produce an indeterminate quantity (e.g., zero divided
by zero) or a complex quantity (e.g., $\sqrt{-1}$). The standard models
these quantities as \emph{not a number}, i.e., a special quantity
denoted \NaN.

We have two sorts of \NaN\ numbers: signaling and quiet. A signalling
\NaN\ raises an exception interrupting computation, a quiet one does
not. The \ieee-754 standard specifies \NaN\ is encoded as $(s,m,e_{\text{max}})$
for $m>0$ and $s\in\{0,1\}$ arbitrary, but does not specify how to
distinguish a quiet from a signaling \NaN. The 2008 revision suggests
\begin{enumerate}
\item For $\beta=2$, the most significant bit of the mantissa should be an
  ``\verb|is_quiet|'' flag (which is 1 for quiet \NaN\ and 0 for signaling)
\item For $\beta=10$, the top five bits of the combination field after
  the sign bit is set to 1. The sixth bit of the field is the ``\verb|is_quiet|''
  flag.
\end{enumerate}
We must stress that different processors have different conventions. For
example, Intel has the second most significant bit of the mantissa set
to 1 for ``quiet \NaN\ Floating-Point Indefinite''.

\begin{defn}
  A \define{Not-a-Number} (or ``\emph{NaN}'') is a floating-point number
  $(s,m,e)$ such that $e=e_{\text{max}}$ and $m>0$.
\end{defn}

\begin{defn}
  Let $(s,m,e_{\text{max}})$ be an \NaN. Then we call $m$ the \define{Payload}.
\end{defn}

Now we will list when \NaN\ will result from a computation. This is not
intended to be exhaustive!

\begin{axiom}
  The quotient of zero with zero, or infinity with infinity, is \NaN:
  \begin{equation}
    \frac{\pm0}{0} = \frac{\pm \infty}{\infty} = \NaN.
  \end{equation}
\end{axiom}

\begin{axiom}
  Multiplying infinity by zero produces a \NaN:
  \begin{equation}
    (\pm 0)\otimes(\infty)=(\pm0)\otimes(-\infty)=\NaN.
  \end{equation}
\end{axiom}

\begin{axiom}
  Adding infinities of different signs (or subtracting infinities of the
  same sign) produces \NaN:
  \begin{subequations}
    \begin{equation}
      \infty\oplus(-\infty)=\infty\ominus\infty=\NaN
    \end{equation}
    \begin{equation}
      (-\infty)\oplus\infty = \NaN.
    \end{equation}
  \end{subequations}
\end{axiom}

\begin{axiom}
If $u$ and $v$ are floating-point numbers and at least one of them is a \NaN,
then $u\oplus v$, $u\ominus v$, $u\otimes v$, $u\oslash v$ all produce \NaN.
\end{axiom}

\begin{axiom}
  In any computation where a complex number should be produced (e.g.,
  $\sqrt{-4}$ and $\sqrt{-\infty}$), a \NaN\ shall be produced.
\end{axiom}

\begin{defn}
  We denote the \define{Quiet NaN} as \qNaN.
\end{defn}

\begin{defn}
  We denote the \define{Signaling NaN} as \sNaN.
\end{defn}

\begin{defn}
  \ieee-754 defines the predicates for any floating-point number $x$,
  \begin{itemize}
  \item $\isNaN(x)$ return ``true'' if and only if $x=\NaN$. 
  \item $\isSignaling(x)$ return ``true'' if and only if $x=\sNaN$.
  \end{itemize}
\end{defn}

\begin{rmk}
  \FORTRAN/-1998 has the \verb|ieee_arithmetic| module\footnote{See,
  e.g., \url{http://fortranwiki.org/fortran/show/ieee_arithmetic}}
  contain \verb|ieee_is_nan(x)| for the $\isNaN(x)$
  predicate, and \CEE/ has \verb|isnan(arg)| defined in \verb|<math.h>|.
\end{rmk}

% chapter 3 of
% Floating-Point Arithmetic and Program Correctness Proofs
% https://ecommons.cornell.edu/handle/1813/6276
% discusses Hoare logic of floating-point

\subsection{Arithmetic Operations}

We will summarize the rules and heuristics for floating-point arithmetic
operators. The exact implementation details depend on the choice of
hardware.

\begin{danger}
  Although we call the following sketches ``algorithms'', they are not
  sufficient for hardware implementers. We suppose there is a sensible
  way to add, subtract, multiply, and divide the mantissas. The
  algorithms produced can easily be transformed into a
  flow-chart. Assertions and comments are placed in [square brackets].
\end{danger}

\begin{notation}
  We write $x\gets v$ to indicate we are assigning the value $v$ to the
  variable $x$. But for assigning multiple values to multiple variables,
  we will write $(x_{1}, x_{2}, \dots, x_{n})\gets(v_{1},v_{2},\dots,v_{n})$
  instead of $x_{1}\gets v_{1}$, $x_{2}\gets v_{2}$, \dots, $x_{n}\gets v_{n}$.
\end{notation}

\subsubsection{Normalization}

We will routinely have a number of steps which should be abstracted away
to a ``normalization'' algorithm.

\algbegin Algorithm N (Normalization of Floating-Point canddiate). Given
a rounding-mode $R$ on floating-point numbers, and
some triple $(s_{x}, m_{x}, e_{x})$ where $m_{x}\in\NN_{0}$ and
$e_{x}\in\ZZ$, try to produce a normalized version $(s_{r}, m_{r}, e_{r})$
such that $\beta^{t-1}\leq m_{r}<\beta^{t}$ and there is an $\ell\in\ZZ$
such that $e_{r}=e_{x}-\ell$ and $e_{\text{min}}\leq e_{r}\leq e_{\text{max}}$
and $m_{r} = \beta^{\ell}m_{x}$. If $e_{x}> e_{\text{max}}$, then the
result is $(-1)^{s_{r}}\infty$. If $e_{x} < e_{\text{min}}$, then a
signed zero is returned.

\algstep N0. [{\it Initialize\/}] Set $\ell\gets0$. Go to step N1.

\algstep N1. If $\beta^{t-1}\leq m_{r}<\beta^{t}$, then go to step N4;
otherwise go to step N2.

\algstep N2. If $m_{r} < \beta^{t-1}$, then determine the number $\ell$
of leading zeros. [We should have $\ell>0$.] Go to step N4.

\algstep N3. If $m_{r} \geq \beta^{t}$, then shift to the right by
$\ell$ places where $\ell = -\max\{k\in\NN | m_{r}\beta^{-k}<\beta^{t}\}$.
[We should have $\ell < 0$.]
Go to step N4.

\algstep N4. Set $(s_{r},m_{r},e_{r})\gets R(s_{r},\beta^{\ell}m_{r},e_{r}+\ell)$.
Go to step N5.

\algstep N5. [{\it Handle overflow\/}] If $e_{r}>e_{\text{max}}$, then
return $(-1)^{s_{r}}\infty$ and terminate the algorithm. Otherwise go to
step N6.

\algstep N6. [{\it Handle underflow\/}] If $e_{r}<e_{\text{min}}$, then
return a signed zero $(-1)^{s_{r}}0$ and terminate the
algorithm. Otherwise, return $(s_{r},m_{r},e_{r})$ and terminate the algorithm.\quad\slug

\subsubsection{Addition}

The algorithm for adding [finite] floating-point numbers $x$ and $y$ is
done in the following steps:

\algbegin Algorithm A (Floating-Point Addition). \label{alg:float-addition}Given two floating
point numbers $x=(s_{x},m_{x},e_{x})$ and $y=(s_{y},m_{y},e_{y})$ ---
both finite numbers --- this
will compute their sum $x\oplus y = (s_{r}, m_{r}, e_{r})$ as a
floating-point number.

\algstep A0.
If $x = 0$, return $y$ and terminate the algorithm.
If $y=0$, then return x and terminate the algorithm.
If $x$ is $\NaN$ or if $y$ is $\NaN$, then return a quiet $\NaN$
and terminate the algorithm.
If $y > x$, then swap $t\gets x$, $x\gets y$, and $y\gets t$.
Now, unless terminated, go to step A1. [Ensures $x\geq y$]

\algstep A1. [{\it Initialize\/}]
Set $s_{r}\gets s_{x}$ the sign of the
result to be the sign of the larger value, and assign $s_{z} \gets \XOR(s_{x},s_{y})$
[so we need to compute $(-1)^{s_{x}}(|m_{x}| + (-1)^{s_{z}}|m_{y}|\beta^{-(e_{x}-e_{y})})$]
Set $e_{r}\gets e_{x}$. Continue to step A2.

\algstep A2. [{\it Significand alignment\/}] Compute $m_{t}\gets m_{y}\beta^{-(e_{x}-e_{y})}$
shifting the mantissa $m_{y}$ to the right by $e_{x}-e_{y}$ digit
positions. Continue to step A3.

\algstep A3. Compute the result significand $m_{r}\gets m_{x} + (-1)^{s_{z}}m_{t}$
where $s_{z}$ depends on the sign of $s_{y}$ and $s_{x}$. If $m_{r}$
is negative (so $s_{z}=1$ and $x>0>-x>y$), it is negated and $s_{r}\gets 1$.
[We have the tenative answer $(s_{r}, m_{r}, e_{r})$] Continue to step A4.

\algstep A4. [{\it Normalize\/}] We call algorithm N on $(s_{r}, m_{r}, e_{r})$
to normalize the sum, and whatever its results are, we return that and
terminate the algorithm.\quad\slug


\subsubsection{Subtraction}

\begin{rmk}[Subtraction]
  Floating-point subtraction $x\ominus y$ is just $x\oplus y'$ where
  $y' = (\XOR(1,s_{y}),m_{y},e_{y})$ is the negated floating-point
  number of $y$.
\end{rmk}

\algbegin Algorithm S (Subtraction). \label{alg:float-subtraction}Given a couple floating-point
numbers $x=(s_{x},m_{x},e_{x})$ and $y=(s_{y}, m_{y}, e_{y})$, produce
their difference.

\algstep S1. [{\it Negate $y$\/}] Set $s_{y}\gets\XOR(s_{y},1)$. Then
continue to S2.

\algstep S2. Sum the two numbers using algorithm A, and return the result.\quad\slug.

\subsubsection{Multiplication} 


\algbegin Algorithm M (Multiplication). \label{alg:float-multiplication}Given a couple floating-point
numbers $x=(s_{x},m_{x},e_{x})$ and $y=(s_{y}, m_{y}, e_{y})$, produce
their product.

\algstep M1. [{\it Short-circuit NaN\/}] If either $x$ or $y$ is an
\NaN, then return \qNaN\ and terminate. Otherwise continue to step M2.

\algstep M2. [{\it Handle infinities\/}] If either $x$ is infinite and
$y=0$ \emph{or} $x=0$ and $y$ is infinite, then return \qNaN\ and
terminate the algorithm.
If $x=(-1)^{s_{x}}\infty$ or $y=(-1)^{s_{y}}\infty$, then set
$s_{r}\gets\XOR(s_{x},s_{y})$, return $(-1)^{s_{r}}\infty$, and
terminate the algorithm.
Otherwise continue to step M3.

\algstep M3. [{\it Short-circuit zeroes\/}] If $x=0$ or $y=0$, then
return 0 and terminate the algorithm.

\algstep M4. [{\it Default case\/}] Otherwise, set
$(s_{r}, m_{r},e_{r})\gets (\XOR(s_{x},s_{y}), m_{x}m_{y}, e_{x}+e_{y})$,
then invoke algorithm N to normalize the product. Return the normalized
product and terminate the algorithm.\quad\slug

\subsubsection{Division}

\algbegin Algorithm D (Division). \label{alg:float-division}Given a couple floating-point
numbers $x=(s_{x},m_{x},e_{x})$ and $y=(s_{y}, m_{y}, e_{y})$, produce
their quotient $x\oslash y$.

\algstep D1. [{\it Short-circuit NaN\/}] If either $x$ or $y$ is an
\NaN, then return \qNaN\ and terminate. If both $x$ and $y$ are zero,
produce a \qNaN\ and terminate. If $\sgn(x)\neq\sgn(y)$ and both are
infinite, return a \qNaN\ and terminate. Otherwise continue to step D2.

\algstep D2. [{\it Short-circuit infinity\/}]
If $\sgn(x)=\sgn(y)$ and both are infinite, return $\sgn(x)\infty$ and terminate.
If $x$ is finite and $y$ is infinite, return 0 and terminate.
If $x$ is infinite and $y\neq0$ is finite, return $\sgn(y)x$ and terminate.
If $x$ is finite and $y=0$, return $\sgn(x)\infty$ and terminate.
Otherwise, continue to step D3.

\algstep D3. Compute $(s_{r},m_{r} + \delta m_{r},e_{r})\gets (\XOR(s_{x},s_{y}),m_{x}/m_{y},e_{x}-e_{y})$
where $0<\delta m_{r}<1$ is a ``guard digit'' used in the rounding
process. Continue to step D4.

\algstep D4. [{\it Normalize\/}] Call algorithm N on
$(s_{r}, m_{r}+\delta m_{r}, e_{r})$ and return its results, terminating
the algorithm.\quad\slug

\begin{xca}[W.M.~Kahan~\cite{kahan1973}; Knuth~\cite{taocp2}]
  Prove that $1\oslash(1\oslash (1\oslash u))=1\oslash u$ for all
  nonzero floating-point numbers $u\neq 0$.
\end{xca}

\begin{rmk}
  Modern computers can perform two $n$-bit integer addition with only
  $\bigO(\log(n))$ circuit depth, the same complexity as two $n$-bit
  integer multiplication. This \emph{does not} translate to the same
  number of CPU cycles! But they are of the same \emph{order of magnitude}.
  Empirically, it seems that the CPU cost for adding two 64-bit
  double-precision floating point numbers is half the cost of multiplying two
  double-precision floating point numbers.

  There have been a number of improvements in performance, e.g., with
  SSE scalar operations.
\end{rmk}


\subsubsection{Standard Model of Floating-Point}

The fundamental axiom of floating-point arithmetic discussed in the
idealized floating-point system carries over to ``real'' \ieee-754
floating-point arithmetic. Higham~\cite[(\S2.2)]{higham2002} calls this the
``standard model of floating-point arithmetic'':

\begin{axiom}[Standard model]
  For any floating-point numbers $u$ and $v$ with machine precision
  $\machinePrec$, we have
  \begin{subequations}
    \begin{equation}
      \fl^{-1}(u~\fl(\mbox{op})~v) = (\fl^{-1}(u)~\mbox{op}~\fl^{-1}(v))(1 + \delta), \quad
      |\delta|\leq\machinePrec
    \end{equation}
    where $\mbox{op}\in\{+,-,\times,/\}$.
  \end{subequations}
\end{axiom}

\begin{rmk}
  In practice, people often include the square root operation as obeying
  the standard model of floating-point arithmetic.
\end{rmk}
\begin{rmk}
  If $x,y\in\RR$ are representable by floating-point numbers $u:=\fl(x)$
  and $v:=\fl(y)$, then we'd have $\fl^{-1}(u)=x(1 + \delta_{x})$ and
  $\fl^{-1}(v)=y(1+\delta_{y})$ where $|\delta_{j}|\leq\machinePrecision$.
  The standard-model of floating-point arithmetic tells us
  \begin{equation}
    \fl^{-1}(u~\mbox{flop}~v) = (x(1+\delta_{x})~\mbox{op}~y(1+\delta_{y}))(1+\delta)
  \end{equation}
  for $|\delta|\leq\machinePrecision$ and $\fl(\mbox{op})=\mbox{flop}$.
\end{rmk}

\marginpar{TODO: write up proof here}
\begin{thm}[Fundamental theorem of floating-point arithmetic]
  The \ieee-754 floating-point arithmetic satisfies the standard model
  of floating-point operations.
\end{thm}

\subsubsection{Costs of Floating-Point Operations}
\begin{heuristic}
  The CPU cycles for each floating-point operation should be estimated
  roughly as follows:
  \begin{center}
  \begin{tabular}{c|c}
    Operation & Run-time\\\hline
    Addition & $A$ \\
    Subtraction & $A$ \\
    Multiplication & $2A$ \\
    Division & $10A$
  \end{tabular}
  \end{center}
  These heuristics are useful when examining and comparing the
  performance of alternative numerical schemes.
\end{heuristic}
\begin{xca}[Research problem]
  Improve on this heuristic. This is a rather ambiguous exercise and
  could be done in several ways.
  
  One possibility: devise lower bounds on the circuit-complexity for
  these operations, and then determine the worst-case, best-case, and
  average-case complexity of each operation.
\end{xca}
\begin{ex}[x86-64 Estimates]
  Empirically, the latency seems to be bounded by the following CPU
  cycles for Intel/AMD x86 architectures:
  \begin{center}
  \begin{tabular}{c|c}
    Operation & Run-time [CPU Cycles]\\\hline
    Addition & $3 \sqrt[4]{2}\approx 3.57$ \\
    Subtraction &  $3\sqrt[4]{2}$ \\
    Multiplication & $\sqrt[4]{125\cdot 6}\approx5.22$ \\
    Division & $21.54$--$34.93$
  \end{tabular}
  \end{center}
  We have simply taken the geometric mean of results reported in Wittman
  and friends~\cite{DBLP:journals/corr/WittmannZHW15}. The estimates
  from the first table are slightly more pessimistic about division and
  multiplication.

  These heuristics should be taken with a grain of salt, since
  floating-point units tend to parallelize computations and the use of
  SSE scalar operations speeds up calculations.
\end{ex}
\begin{ex}[Complications due to optimizations]
  Consider a hypothetical CPU which takes 4 cycles to perform
  addition and 1 CPU cycle to load a memory cell into one of its
  registers. What modern CPU pipelines do is run several computations in
  parallel. For example, the CPU pipeline would evaluate $a+b+c+d+e+f+g$ as:
  \begin{center}
    \begin{longtable}{r|l}
      Cycle 1  & load $a$ into FPU\\\hline
      Cycle 2  & load $b$ into FPU\\\hline
      Cycle 3  & compute $a+b$ (cycle $1/4$)\\\hline
      Cycle 4  & load $c$ into FPU,\\
               & continue $a+b$ ($2/4$)\\\hline
      Cycle 5  & load $d$ into FPU,\\
               & continue $a+b$ ($3/4$)\\\hline
      Cycle 6  & compute $c+d$ (1/4),\\
               & finish $a+b$ ($4/4$) and store in top of stack as $s$\\\hline
      Cycle 7  & load $e$ into FPU,\\
               & continue $c+d$ ($2/4$)\\\hline
      Cycle 8  & compute $e+s$ ($1/4$)\\
               & continue $c+d$ ($3/4$)\\\hline
      Cycle 9  & load $g$ into FPU,\\
               & continue $e+s$ ($2/4$),\\
               & finish $c+d$ ($4/4$) and store it in top of stack as $t$\\\hline
      Cycle 10 & start $g+t$ ($1/4$)\\
               & Continue $e+s$ ($3/4$)\\\hline
      Cycle 11 & Continue $g+t$ ($2/4$)\\
               & Finish $e+s$ ($4/4$) and store in top of stack as $u$\\\hline
      Cycle 12 & Continue $g+t$ ($3/4$)\\\hline
      Cycle 13 & Finish $g+t$ ($4/4$) and store it in top of stack as $v$\\\hline
      Cycle 14 & Start $u+v$ ($1/4$)
    \end{longtable}
  \end{center}
  The CPU will finish after 3 more cycles, giving us a grand total of 17
  cycles. But if we just na\"{\i}vely count the 6 addition operations
  and 7 memory loads, then we'd get $6\times4 + 7=31$ CPU cycles.
\end{ex}

\input{tex/float/identities}
%% \subsubsection{Identities of Floating-Point Arithmetic}

%% From the algorithms we have thus described, we have a number of useful
%% identities. This discussion stems from reading Knuth~\cite{taocp2}.

%% \begin{thm}
%%   We have the following identities for any floating-point numbers
%%   $u,v\in\mathbb{F}$:
%%   \begin{subequations}
%%     \begin{gather}
%%       u\oplus v = v\oplus u;\\
%%       u\ominus v = u \oplus -v;\\
%%       -(u\oplus v) = -u \oplus -v;\\
%%       u\oplus v=0\quad\mbox{if and only if}\quad v=-u;\\
%%       u\oplus 0=u.
%%     \end{gather}
%%     If further $u\oplus v=u+v$ is exact, then
%%     \begin{equation}
%%       (u\oplus v)\ominus v=u.
%%     \end{equation}
%%   \end{subequations}
%% \end{thm}
%% \begin{thm}
%%   If $u\leq v$ are arbitrary floating-point numbers, then for any
%%   floating-point number $w$ we have
%%   \begin{equation}
%%     u\oplus w\leq v\oplus w.
%%   \end{equation}
%% \end{thm}

%% \begin{thm}
%%   For any floats $u$ and $v$, we have
%%   \begin{subequations}
%%     \begin{gather}
%%       u\otimes v = v\otimes u,\\
%%       (-u)\otimes v = u\otimes(-v) =  -(u\otimes v)\\
%%       1\otimes v = v\otimes 1 = v\\
%%       u\otimes v = 0\quad\mbox{if and only if}\quad u=0\mbox{ or }v=0.
%%     \end{gather}
%%   \end{subequations}
%% \end{thm}
%% \begin{thm}[Monotony law for multiplication]
%%   If $u\leq v$ are floating-point numbers and $w>0$ is a floating-point
%%   number, then $u\otimes w\leq v\otimes w$.
%% \end{thm}
%% \begin{thm}
%%   For any floating-point numbers $u$ and $v$, we have
%%   \begin{subequations}
%%     \begin{gather}
%%       (-u)\oslash v = u\oslash(-v)=-(u\oslash v);\\
%%       0\oslash v = 0,\\
%%       u\oslash 1 = u,\\
%%       u\oslash u = 1.
%%     \end{gather}
%%     Further, if $u\otimes v=u\times v\neq0$ is exact and nonzero, then
%%     \begin{equation}
%%       (u\otimes v)\oslash v=u.
%%     \end{equation}
%%   \end{subequations}
%% \end{thm}
%% \begin{thm}[Monotony laws of division]
%%   For any floating point numbers $u\leq v$ and $w>0$, then
%%   \begin{subequations}
%%     \begin{equation}
%%       u\oslash w\leq v\oslash w
%%     \end{equation}
%%     and further, when $0 < u \leq v$, we have
%%     \begin{equation}
%%       w\oslash u\geq w\oslash v
%%     \end{equation}
%%   \end{subequations}
%% \end{thm}
%% \begin{prop}[Failure of Cauchy's law]
%% Let $x_{1},\dots,x_{n},y_{1},\dots,y_{n}$ be floating-point
%% numbers. Then, in general, Cauchy's law fails to hold:
%% \begin{equation}
%%   (x_{1}^{2}\oplus\dots\oplus x_{n}^{2})\otimes(y_{1}^{2}\oplus\dots\oplus y_{n}^{2})\not\geq
%%   (x_{1}\otimes y_{1})\oplus(\dots)\oplus(x_{n}\otimes y_{n}).
%% \end{equation}
%% Here $x_{j}^{2}=(x_{j}\otimes x_{j})$ and similarly for $y_{j}$, for $j=1,\dots,n$.
%% \end{prop}

%% \subsection{Relational Operators}

%% We seldom have ``exact equality'' of floating-point numbers, instead
%% resorting to ``approximate equality'' parametrized by some tolerance
%% $\varepsilon_{\text{tol}}$. The concepts discussed here may be found in Knuth~\cite{taocp2}.

%% \begin{defn}
%%   Given some $\varepsilon_{\text{tol}}>0$,
%%   the relational operators comparing two floating-point numbers
%%   $x=(s_{x},m_{x},e_{x})$ and $y=(s_{y},m_{y},e_{y})$ are:
%%   \begin{enumerate}
%%   \item \define{Definitely less than} $u\flLt v$ if and only if
%%     $v-u > \varepsilon_{\text{tol}}\max(\beta^{e_{x}},\beta^{e_{y}})$
%%   \item \define{Approximately equal to} $u\flApprox v$ if and only if
%%     $|v-u|\leq\varepsilon_{\text{tol}}\max(\beta^{e_{x}},\beta^{e_{y}})$
%%   \item \define{Definitely greater than} $u\flGt v$ if and only if
%%     $u-v > \varepsilon_{\text{tol}}\max(\beta^{e_{x}},\beta^{e_{y}})$
%%   \item \define{Essentially equal to} $u\flEq v$ if and only if
%%     $|v-u|\leq\varepsilon_{\text{tol}}\min(\beta^{e_{x}},\beta^{e_{y}})$
%%   \end{enumerate}
%% \end{defn}
%% \begin{notation}
%%   We may write the tolerance subscripted to these relational operators,
%%   e.g., $u\flLt_{\varepsilon}v$; or we may write to the right in
%%   parentheses $u\flLt v~(\varepsilon)$.
%% \end{notation}
%% \begin{rmk}
%%   Observe $u\flLt v$ is the same as $v\flGt u$. Further, observe that
%%   $u\flEq v$ is a stronger condition than $u\flApprox v$.
%% \end{rmk}
%% \begin{rmk}
%%   The {\tt fcmp} library\footnote{\url{http://fcmp.sourceforge.net/}} has implemented the first three relational
%%   operators in a single function in \CEE/: \verb|fcmp(x,y,delta)| returns $-1$
%%   if $x\flLt_{\delta} y$, 0 if $x\flApprox_{\delta} y$, and $+1$ if $x\flGt_{\delta} y$.
%% \end{rmk}
%% \begin{thm}
%%   For any floating-point numbers $u$, $v$, and $\varepsilon>0$, we have
%%   the following identities:
%%   \begin{gather}
%%     u\flLt_{\varepsilon} v\quad\implies\quad v\flGt_{\varepsilon} u\\
%%     u\flEq_{\varepsilon} v\quad\implies\quad u\flApprox_{\varepsilon}v\\
%%     u\flEq_{\varepsilon} u\\
%%     u\flLt_{\varepsilon} v\implies u < v\\
%%     u\flLt_{\varepsilon_{1}}v\quad\mbox{and}\quad \varepsilon_{1}<\varepsilon_{2}\quad\implies\quad
%%     u\flLt_{\varepsilon_{2}}v
%%   \end{gather}
%% \end{thm}
%% \begin{thm}[Transitivity law for float inequality]
%%   For any $u\flLt_{\varepsilon_{1}}v$ and $v\flLt_{\varepsilon_{2}}w$,
%%   we have $u\flLt_{\varepsilon_{3}}w$ where $\varepsilon_{3}=\min(\varepsilon_{1},\varepsilon_{2})$.
%% \end{thm}
%% \begin{thm}[Transitivity for essential equality]
%%   For any $u\flEq_{\varepsilon_{1}}v$ and $v\flEq_{\varepsilon_{2}}w$,
%%   we have $u\flApprox_{\varepsilon_{3}}w$ where $\varepsilon_{3}=\varepsilon_{1}+\varepsilon_{2}$.
%% \end{thm}
%% \begin{thm}
%%   For any floating-point numbers $x$, $y$, if $|x-y|\leq\varepsilon|x|$
%%   \emph{and} $|x-y|\leq\varepsilon|y|$, then we have $x\flEq_{\varepsilon}y$.
%% \end{thm}
%% \begin{thm}
%%   For any floating-point numbers $x$, $y$, if $|x-y|\leq\varepsilon|x|$
%%   \emph{or} $|x-y|\leq\varepsilon|y|$, then we have $x\flApprox_{\varepsilon}y$.
%% \end{thm}


%% \begin{thm}[Essential associativity]
%%   For any floating-point numbers $x$, $y$, $z$, and any
%%   $\varepsilon\geq 2\machEps/(1 - \machEps/2)^{2}$, we have
%%   \begin{equation}
%%     (x\otimes y)\otimes z\flEq_{\varepsilon}x\otimes(y\otimes z).
%%   \end{equation}
%% \end{thm}

%% \begin{xca}
%%   Prove or find a counter example: for any floating-point numbers $x$,
%%   $y$, and $z$, there is a $\varepsilon_{0}>0$ such that for any
%%   $\varepsilon>\varepsilon_{0}$ we have $(x\oplus y)\oplus z\flEq_{\varepsilon}x\oplus(y\oplus z)$.
%% \end{xca}

%% \begin{xca}[{H.~Bj\"ork; Kahan~\cite[{(item 5, pg 0-2)}]{kahan1973}; Knuth~\cite[(\S4.2.2,ex.15)]{taocp2}}]
%%   Does the computed midpoint of an interval always lie between the
%%   endpoints? That is, for floating-point numbers $x$ and $y$, does
%%   $x\leq y$ imply $x\leq (x\oplus y)\oslash 2\leq y$? [Hint: consider
%%     $\beta=2$ as well as other bases $\beta>2$.]
%% \end{xca}

%% % William Kahan, Implementation of Algorithms, part I, lecture notes, 1973