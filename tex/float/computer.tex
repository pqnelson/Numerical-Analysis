We can now refine our model of floating-point arithmetic to factor in
the aspects we simplified:
\begin{enumerate}
\item Exponents range from $e_{\text{min}}$ to $e_{\text{max}}$ (finite integers)
\item Infinities are encoded as numbers (denoted $+\infty$ and $-\infty$)
\item Invalid operations are encoded as special quantities, denoted
  ``\NaN'', called ``Not-a-Number''
\end{enumerate}
Everything that has been said so far still holds for actual \ieee-754
floating-point numbers (non-associativity, the fundamental axiom of
floating-point, rounding schemes, etc.). But the algorithm and proof may
no longer be correct when underflowing occurs.

Further, the arithmetic operations may be given a more concrete
implementation, and we have grounds for preferring a radix $\beta=2$.

\input{tex/float/extended-reals}

\subsection{Defining Numbers}
\begin{defn}
  Let $\beta>1$ be a fixed integer, $t>0$ an integer,
  $e_{\text{min}}<-t<0<t<e_{\text{max}}$ be
  fixed integers. Then we define the set of \define[Floating-Point Number]{Floating-Point Numbers}
  in radix $\beta$ to be the set
  \begin{equation}
    \mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}} = \{(s,m,e)\in\ZZ^{3}|
    s\in\{0,1\}, 0\leq m<\beta^{t}, e_{\text{min}}\leq e\leq e_{\text{max}}\}.
  \end{equation}
  We call $m$ the \define{Mantissa} (or \emph{Significand}), $s$ the
  \define{Sign}, and $e$ the \define{Exponent}.
\end{defn}


\begin{danger}
  In practice, it's not uncommon to interpret the mantissa $m$ as
  $f=m/\beta^{t}$ a fraction between $1/\beta\leq f<1$. The \ieee-754
  standard treats floating-point numbers in this manner, as does
  Knuth~\cite{taocp2}. However, the presentation of material becomes
  rather complicated when working with a fraction $f$ instead of an
  integer $m$. For this reason, we will continue working with the
  mantissa $m$ as an integer.
\end{danger}


\begin{ddanger}
  We should observe that any number $x\in\RR$ such that $|x|<\beta^{e_{\text{min}}}$
  or $|x|>\beta^{e_{\text{max}}}$ cannot be represented in
  the floating-point system. What happens for
  $|x|>\beta^{e_{\text{max}}}$, such numbers are rounded to $\pm\infty$
  (with the same sign as $x$). For tiny numbers $|x|<\beta^{e_{\text{min}}}$,
  we either use subnormal numbers $x=\pm m\times\beta^{e_{\text{min}}-t}$
  or for even smaller numbers\dots we just round $x$ down to zero.
\end{ddanger}


\begin{rmk}\index{Double-Precision!Exponent Bounds}
  The \ieee-754 standard specifies the following bounds for $\beta=2$:
  \begin{center}
    \bgroup
    \def\arraystretch{1.125}
    \begin{tabular}{|c|c|c|c|}
      Name   & $e_{\text{min}}$ & $e_{\text{max}}$ & $t$ \\\hline
      Single & $-126$         & $127$          & $24$\\
      Double & $-1022$        & $1023$         & $53$\\
      Quad   & $-16382$       & $16383$        & $113$\\
      Octuple & $-262142$     & $262143$       & $237$
    \end{tabular}
    \egroup
  \end{center}
  Most contemporary hardware (as of 2021) supports single and
  double-precision floating-point, and have begun to support
  quad-precision arithmetic (e.g., Intel i5-4440S apparently supports
  it, or more precisely: \textsc{gfortran} $9.3.0$ supports
  it). \emph{Double-precision should be the default} (unless speed is important
  and precision is less important, in which single-precision may be used).
\end{rmk}

\begin{chunk}
  We can write a simple \FORTRAN/ subroutine to print the binary
  components of an \ieee-754 double-precision
  number:\marginpar{\texttt{utils.f90}}\index{binary-print-double@\lstinline{binary_print_double()}}
  \begin{fortran}
  subroutine binary_print_double(x)
    use ISO_FORTRAN_ENV
    implicit none
    real(real64), intent(in) :: x
    character(len=64) :: bin
    
    write(bin, '(B64.64)') x
    
    print *, 'Sign Exponent     Mantissa '
    print *, bin(1:1), '    ', bin(2:12), '  ', bin(13:64)
  end subroutine
  \end{fortran}
\end{chunk}


\begin{defn}
  We call a number $r\in\RR$ \define[Floating-Point Number!Representable]{Representable} in
  $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$ if there is a
  number $u\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$ such
  that
  \begin{equation}
    u = \fl(r).
  \end{equation}
  Further, we say $r$ is \define[Floating-Point Number!Exactly Representable]{Exactly Representable} if
  \begin{equation}
    r = (-1)^{s}\frac{m}{\beta^{t}}\beta^{e}
  \end{equation}
  for some $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$.
\end{defn}


\begin{defn}
  Floating-point numbers with $\beta=2$ and $e>e_{\text{min}}$, we may
  assume the mantissa $m$ has implicitly a leading-bit of 1,
  called the \define[Floating-Point Number!Implicit Leading Bit]{Implicit Leading Bit}.
\end{defn}


\begin{rmk}
  \ieee-754 specifies $\beta=2$ or $\beta=10$. Numerous studies~\cite{DBLP:journals/corr/abs-1004-3374,10.1145/363235.363240,5009112,10.1145/362003.362013,4039164}
have found $\beta=2$ with an implicit leading bit has better worst-case
and average-case accuracy than other choices of $\beta$. Although, there
is a theoretical case to be made for $\beta=\E\approx2.71828\dots$ as a
choice of radix, its implementation seems impossible.
\end{rmk}


\begin{defn}\label{def:computer-float:normal-subnormal-denormal}
We call a representable floating-point number $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
\begin{itemize}
\item\index{Normal Number}\define[Floating-Point Number!Normal]{Normal} if $\beta^{t-1}\leq m<\beta^{t}$ or $m=e=0$;
\item\index{Subnormal Number}\define[Floating-Point Number!Subormal]{Subnormal} if $e=e_{\text{min}}$ and $0<m<\beta^{t-1}$;
\item All others are \define{Denormalized}.
\end{itemize}
\end{defn}


\begin{thm}
  Every representable number in $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
  is either normal or subnormal, but not both.
\end{thm}
\begin{proof}
  Let  $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$ be
  arbitrary. By
  definition~\ref{def:computer-float:normal-subnormal-denormal},
  it's either normal, subnormal, or denormal. We need to consider the
  denormal situation.

  \textsc{Case 1:} If $m=0$, then we have two subcases.

  {}\quad\textsc{Subcase 1:} If $e=0$, then we have $(0,0,0)=0$ which is
  normal.

  {}\quad\textsc{Subcase 2:} If $e\neq0$, then $(0,0,e)=\fl(\beta^{e})$
  which can be encoded as $(0,\beta^{t-1}, e-(t-1))$ which is normal for
  $e_{\text{min}}<e-(t-1)<e_{\text{max}}$ and rounds to zero or infinity
  otherwise.

  \textsc{Case 2:} If $m\neq0$, then $m<\beta^{t-1}$, so it's ``leading
  digit'' is 0. If we write the base-$\beta$ expansion
  \begin{equation}
    m=\sum^{t-1}_{i=0}d_{i}\beta^{i}
  \end{equation}
  and suppose $d_{t-1}=\dots=d_{t-k}=0$ but $d_{t-(k+1)}\neq0$ (for some
  $k\geq1$), then $(s,m\beta^{\ell},e-\ell)$ where $\ell=\min\{k,e-e_{\text{min}}\}$
  is either normal (when $\ell=k$) or subnormal (when $\ell<k$).
\end{proof}

\begin{defn}
Let $(s,m,e)\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
have $0<m<\beta^{t-1}$. Then we define the transformation
\begin{equation}
  (s,m,e)\to(s,m\beta^{\ell},e-\ell)
\end{equation}
where $\ell$ is the number of leading zeroes of the mantissa, the
transformation is defined as \define{Normalization}.
\end{defn}

\begin{rmk}
In other words, any denormalized floating-point number can be normalized
into a subnormal or normal.
\end{rmk}


\begin{thm}
The normal and subnormal floating-point representations are unique.
\end{thm}
\begin{proof}
\textsc{Case 1:} for subnormal floating-point numbers, they are clearly
unique since it amounts to a choice of sign and mantissa.

\textsc{Case 2:} for normal floating-point numbers, suppose
$(s_{1},m_{1},e_{1})$ and $(s_{2},m_{2},e_{2})$ encode the same number
\begin{equation}
  x = (-1)^{s_{1}}(m_{1}/\beta^{t})\beta^{e_{1}} = (-1)^{s_{2}}(m_{2}/\beta^{t})\beta^{e_{2}}.
\end{equation}
Well, necessarily $s_{1}=s_{2}$ and $\beta^{t-1}\leq m_{j} < \beta^{t}$.
We see then that
\begin{equation}
  m_{1}\beta^{e_{1}-t} = m_{2}\beta^{e_{2}-t}
\end{equation}
imply
\begin{equation}
  m_{1} = m_{2}\beta^{e_{2}-e_{1}}.
\end{equation}
But for the bounds to be satisfied, $e_{2}=e_{1}$, which then implies
$m_{1}=m_{2}$.

Hence any $x\in\RR$ representable as a floating-point number, has a unique
representation $y\in\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$.
\end{proof}


\begin{defn}
  The largest finite floating-point number in $\mathbb{F}_{\beta,t,e_{\text{min}},e_{\text{max}}}$
  is denoted\index{$\Omega$}\index{Floating-Point Number!Omega@{$\Omega$}}
  \begin{equation}
    \Omega := (\beta - \beta^{1-t})\beta^{e_{\text{max}}}.
  \end{equation}
  The smallest positive subnormal number is denoted\index{$\alpha$}\index{Floating-Point Number!alpha@{$\alpha$}}
  \begin{equation}
    \alpha := \beta^{e_{\text{min}}-t+1}.
  \end{equation}
\end{defn}

\input{tex/float/computer/precision}
\input{tex/float/computer/infinity}
\input{tex/float/computer/nan}

\input{tex/float/computer/arithmetic}

\input{tex/float/identities}

% chapter 3 of
% Floating-Point Arithmetic and Program Correctness Proofs
% https://ecommons.cornell.edu/handle/1813/6276
% discusses Hoare logic of floating-point

% William Kahan, Implementation of Algorithms, part I, lecture notes, 1973
