\section{Conjugate Gradient Method}

The ``trick'' to the conjugate gradient method is a standard one
deployed by mathematicians: change the problem to one more palatable to
a different toolkit. However, the real intuition is to use
Graham-Schmidt on the residual vectors in an iterative procedure.

\begin{theorem}
Let $A$ be a positive definite matrix, and $\vec{b}$ be a vector.
Then $\vec{x}_{*}$ solves $A\vec{x}=\vec{b}$ if and only if
$\vec{x}_{*}$ minimizes
\begin{equation}
g(\vec{x})=\langle\vec{x},A\vec{x}\rangle-2\langle\vec{x},\vec{b}\rangle.
\end{equation}
\end{theorem}

\begin{proof}
Obvious.
\end{proof}

Let $\vec{x}^{(0)}$ be an initial guess for $\vec{x}_{*}$. If
$A\vec{x}^{(0)}=\vec{b}$, then we're done. We'll assume this is not the
case. Let
$\vec{v}^{(1)}\neq\vec{0}$ be an initial search direction. Compute
\begin{equation}
t_{1} = \frac{\langle\vec{v}^{(1)},\vec{b}-A\vec{x}^{(0)}\rangle}{\langle\vec{v}^{(1)},A\vec{v}^{(1)}\rangle},
\end{equation}
then set
\begin{equation}
\vec{x}^{(1)} = \vec{x}^{(0)}+t_{1}\vec{v}^{(1)}.
\end{equation}
Now, look, $t_{1}$ describes the overlap of the residual vector
$\vec{r}^{(0)}=\vec{b}-A\vec{x}^{(0)}$ in the direction of
$\vec{v}^{(1)}$, so what $\vec{x}^{(1)}$ computes is the correction in
the $\vec{v}^{(1)}$ direction to minimize the residual vector's
$\vec{v}^{(1)}$ component. This leads to
$g(\vec{x}^{(1)})<g(\vec{x}^{(0)})$.

If we had access to $n$ direction vectors $\vec{v}^{(1)}$, \dots,
$\vec{v}^{(n)}$ which are $A$-orthogonal,
\begin{equation}
\langle\vec{v}^{(i)},A\vec{v}^{(j)}\rangle=0\quad\mbox{if }i\neq j,
\end{equation}
then we can iterative step by taking
\begin{equation}
\vec{x}^{(k)} = \vec{x}^{(k-1)} + t_{k}\vec{v}^{(k)},
\end{equation}
where
\begin{equation}
t_{k} = \frac{\langle\vec{v}^{(k)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}.
\end{equation}
Since we keep moving towards the direction which systematically reduces
the residual vector, one dimension at a time, this leads us to conclude
$g(\vec{x}^{(k)})<g(\vec{x}^{(k-1)})$. 
The problem before us now is how to pick the $A$-orthogonal direction
vectors $\vec{v}^{(k)}$?

We see that
\begin{equation}
\langle\vec{r}^{(k-1)},\vec{v}^{(i)}\rangle=0
\end{equation}
for $i=1,$ $2$, \dots, $k-1$. Then we can use the residual vector
$\vec{r}^{(k-1)}$ to determine $\vec{v}^{(k)}$ by
\begin{subequations}
\begin{equation}
\vec{v}^{(k)}=\vec{r}^{(k-1)}+s_{k-1}\vec{v}^{(k-1)},
\end{equation}
and now we just need to determine what $s_{k-1}$. Now we just need to
pick $s_{k-1}$ to make $\vec{v}^{(k)}$ ``$A$-orthogonal'' to
$\vec{v}^{(k-1)}$,
\begin{equation}
\langle\vec{v}^{(k-1)},A\vec{v}^{(k)}\rangle=0.
\end{equation}
Since
\begin{equation}
A\vec{v}^{(k)}=A\vec{r}^{(k-1)}+s_{k-1}A\vec{v}^{(k-1)},
\end{equation}
and
\begin{equation}
0=\langle\vec{v}^{(k-1)},A\vec{v}^{(k)}\rangle=
\langle\vec{v}^{(k-1)},A\vec{r}^{(k-1)}\rangle
+s_{k-1}\langle\vec{v}^{(k-1)},A\vec{v}^{(k-1)}\rangle,
\end{equation}
we can solve this to find
\begin{equation}\label{eq:iterative:conjugate-gradient:sk-init}
s_{k-1} = -\frac{\langle\vec{v}^{(k-1)},A\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k-1)},A\vec{v}^{(k-1)}\rangle}.
\end{equation}
\end{subequations}
It can be shown that $\langle\vec{v}^{(k)},A\vec{v}^{(j)}\rangle=0$ for
$i=1$, \dots, $k-1$.

The goal now will be to express $s_{k}$ and $t_{k}$ using the residual
vectors $\vec{r}^{(k)}$ and $\vec{r}^{(k-1)}$.

We see that, by direct calculation,
\begin{subequations}
\begin{align}
t_{k} &= \frac{\langle\vec{v}^{(k)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}\\
&= \frac{\langle\vec{r}^{(k-1)}+s_{k-1}\vec{v}^{(k-1)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}\\
&= \frac{\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}
+ s_{k-1}\frac{\langle\vec{v}^{(k-1)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}
\end{align}
\end{subequations}
Since $\langle\vec{v}^{(k-1)},\vec{r}^{(k-1)}\rangle=0$, we conclude
\begin{equation}\label{eq:iterative:conjugate-gradient:tk}
\boxed{t_{k} = \frac{\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}.} 
\end{equation}
Great, this allows us to use the residual vectors to determine the $t_{k}$.

Now, we will determine $s_{k}$ using residual vectors, and this will
enable us to find the direction vectors. We begin by remembering
\begin{subequations}
\begin{equation}
\vec{x}^{(k)}=\vec{x}^{(k-1)}+t_{k}\vec{v}^{(k)}.
\end{equation}
Then we compute $\vec{r}^{(k)}$ the residual vector by multiplying both
sides on the left by $A$ and subtracting out by $\vec{b}$,
\begin{equation}
A\vec{x}^{(k)}-\vec{b}=A\vec{x}^{(k-1)}-\vec{b}+t_{k}A\vec{v}^{(k)},
\end{equation}
hence
\begin{equation}
\vec{r}^{(k)}=\vec{r}^{(k-1)}+t_{k}A\vec{v}^{(k)}.
\end{equation}
We have
\begin{equation}
  \langle\vec{r}^{(k)},\vec{r}^{(k)}\rangle
  =\langle\vec{r}^{(k-1)}-t_{k}A\vec{v}^{(k)},\vec{r}^{(k)}\rangle
  =-t_{k}\langle A\vec{v}^{(k)},\vec{r}^{(k)}\rangle.
\end{equation}
Now we use Eq~\eqref{eq:iterative:conjugate-gradient:tk} to give us
another relationship,
\begin{equation}
\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle=t_{k}\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle.
\end{equation}
Then we have, starting with Eq~\eqref{eq:iterative:conjugate-gradient:sk-init},
\begin{align}
s_{k}
&=-\frac{\langle\vec{v}^{(k)},A\vec{r}^{(k)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}\\
&=-\frac{\langle\vec{r}^{(k)},A\vec{v}^{(k)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}\\
&=\frac{(1/t_{k})\langle\vec{r}^{(k)},A\vec{r}^{(k)}\rangle}{(1/t_{k})\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}\\
&=\frac{\langle\vec{r}^{(k)},A\vec{r}^{(k)}\rangle}{\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}.
\end{align}
\end{subequations}
This culminates our derivation of the conjugate gradient method.

In summary, we have the conjugate gradient method be given by the
following equations:
\begin{align}
t_{k} &= \frac{\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}{\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}\\
\vec{x}^{(k)} &= \vec{x}^{(k-1)} + t_{k}\vec{v}^{(k)}\\
\vec{r}^{(k)} &= \vec{r}^{(k-1)} - t_{k}A\vec{v}^{(k)}\\
s_{k} &= \frac{\langle\vec{r}^{(k)},A\vec{r}^{(k)}\rangle}{\langle\vec{r}^{(k-1)},\vec{r}^{(k-1)}\rangle}\\
\vec{v}^{(k+1)} &= \vec{r}^{(k)} + s_{k}\vec{v}^{(k)}.
\end{align}

\subsection{Conditioned Conjugate Gradient Method}

If the matrix $A$ is ill-conditioned, then the conjugate gradient method
is highly susceptible to rounding errors. The trick is to select a
nonsingular ``conditioning matrix'' $C$ such that
\begin{equation}
\widetilde{A}=C^{-1}A\transpose{(C^{-1})}
\end{equation}
is better conditioned. Then our system of equations we would want to
solve would be
\begin{subequations}
\begin{equation}
\widetilde{A}\widetilde{\vec{x}}=\widetilde{\vec{b}}
\end{equation}
or
\begin{equation}
(C^{-1}A\transpose{(C^{-1})})(\transpose{C}\vec{x})=C^{-1}\vec{b}.
\end{equation}
\end{subequations}
Usually it's desirable to take $C$ to be a diagonal matrix, e.g., if
$A=D+L+U$ has all nonzero diagonal entries, then take the component-wise
inverse squareroot of the absolute value of the diagonal entries
$C=|D|^{-1/2}$.

\textbf{However,} instead of solving for $\widetilde{\vec{x}}$ and then
multiplying by $\transpose{(C^{-1})}$, we can incorporate the
conditioning \emph{into} the conjugate gradient algorithm
\emph{implicitly}.

We find by direct computation and recalling
$\widetilde{\vec{x}}^{(k)}=\transpose{C}\vec{x}^{(k)}$:
\begin{calculation}
  \widetilde{\vec{r}}^{(k)}
\step{definition of residual}
  \widetilde{\vec{b}} - \widetilde{A}\widetilde{\vec{x}}^{(k)}
\step{unfolding tilde notation}
  C^{-1}\vec{b}-(C^{-1}A\transpose{(C^{-1})})(\transpose{C}\vec{x}^{(k)})
\step{since $I=\transpose{(C^{-1})}\transpose{C}$, distributivity}
  C^{-1}(\vec{b}-A\vec{x}^{(k)})
\step{definition of residual}
  C^{-1}\vec{r}^{(k)}.
\end{calculation}
We therefore denote the residual vector for the conditioned system
$\vec{w}^{(k)}=C^{-1}\vec{r}^{(k)}$. \emph{This will need to be computed for each step.}

We want to determine the scalar $\widetilde{s}_{k}$ to pick the next
direction vector $\widetilde{\vec{v}}^{(k)} = \transpose{C}\vec{v}^{(k)}$.
As before,
\begin{equation}
\boxed{\widetilde{s}_{k} = \frac{\langle\vec{w}^{(k)},\vec{w}^{(k)}\rangle}{\langle\vec{w}^{(k-1)},\vec{w}^{(k-1)}\rangle}}
\end{equation}
where $\vec{w}$ is the residual vector.

The scalar $\widetilde{t}_{k}$ is computed similarly, if we just naively
put tildes on things we would expect
\begin{calculation}
  \widetilde{t}_{k}
\step{unfolding $t_{k}$}
  \frac{\langle\widetilde{\vec{r}}^{(k-1)},\widetilde{\vec{r}}^{(k-1)}\rangle}{\langle\widetilde{\vec{v}}^{(k)},A\widetilde{\vec{v}}^{(k)}\rangle}
\step{unfolding tildes}
  \frac{\langle C^{-1}\vec{r}^{(k-1)}, C^{-1}\vec{r}^{(k-1)}\rangle}%
     {\langle\transpose{C}\vec{v}^{(k)},C^{-1}A(\transpose{C})^{-1}\transpose{C}\widetilde{v}^{(k)}\rangle}
\step{since $(\transpose{C})^{-1}\transpose{C}=I$, associativity}
  \frac{\langle C^{-1}\vec{r}^{(k-1)}, C^{-1}\vec{r}^{(k-1)}\rangle}%
     {\langle\transpose{C}\vec{v}^{(k)},C^{-1}A\vec{v}^{(k)}\rangle}
\step{folding in $\vec{w}^{(k-1)}$}
  \frac{\langle \vec{w}^{(k-1)}, \vec{w}^{(k-1)}\rangle}%
     {\langle\transpose{C}\vec{v}^{(k)},C^{-1}A\vec{v}^{(k)}\rangle}
\step{since $C$ is real and symmetric}
  \frac{\langle \vec{w}^{(k-1)}, \vec{w}^{(k-1)}\rangle}%
     {\langle\vec{v}^{(k)},CC^{-1}A\vec{v}^{(k)}\rangle}
\step{since $CC^{-1}=I$}
  \frac{\langle \vec{w}^{(k-1)}, \vec{w}^{(k-1)}\rangle}%
     {\langle\vec{v}^{(k)},A\vec{v}^{(k)}\rangle}
\end{calculation}
thus giving us
\begin{equation}
\boxed{\widetilde{t}_{k} = \frac{\langle \vec{w}^{(k-1)}, \vec{w}^{(k-1)}\rangle}%
     {\langle\vec{v}^{(k)},A\widetilde{v}^{(k)}\rangle}.}
\end{equation}
Similarly, from
\begin{equation*}
\widetilde{\vec{x}}^{(k)}=\widetilde{\vec{x}}^{(k-1)} + \widetilde{t}_{k}\widetilde{\vec{v}}^{(k)},
\end{equation*}
we have
\begin{equation*}
\transpose{C}\vec{x}^{(k)}=\transpose{C}\vec{x}^{(k-1)} + \widetilde{t}_{k}\transpose{C}\vec{v}^{(k)},
\end{equation*}
and therefore multiplying on both side from the left by $(\transpose{C})^{-1}$,
\begin{equation}
\boxed{\vec{x}^{(k)}=\vec{x}^{(k-1)} + \widetilde{t}_{k}\vec{v}^{(k)}.}
\end{equation}
Similarly, we can find
\begin{calculation}
  \widetilde{\vec{r}}^{(k)} = \widetilde{\vec{r}}^{(k-1)} - \widetilde{t}_{k}\widetilde{A}\widetilde{\vec{v}}^{(k)}
\step[\equiv]{unfolding tildes}
  C^{-1}\vec{r}^{(k)} = C^{-1}\vec{r}^{(k-1)} - \widetilde{t}_{k} C^{-1}A\transpose{(C^{-1})}\transpose{C}\vec{v}^{(k)}
\step[\equiv]{multiply through by $C$ on the left}
  \vec{r}^{(k)} = \vec{r}^{(k-1)} - \widetilde{t}_{k} A\transpose{(C^{-1})}\transpose{C}\vec{v}^{(k)}
\step[\equiv]{since $I=\transpose{(C^{-1})}\transpose{C}$}
  \vec{r}^{(k)} = \vec{r}^{(k-1)} - \widetilde{t}_{k} A\vec{v}^{(k)}
\end{calculation}
Hence
\begin{equation}
\boxed{\vec{r}^{(k)} = \vec{r}^{(k-1)} - \widetilde{t}_{k} A\vec{v}^{(k)}.}
\end{equation}
Finally, we find the next direction vector $\vec{v}^{(k+1)}$, again by
direct calculation
\begin{calculation}
  \widetilde{\vec{v}}^{(k+1)}=\widetilde{\vec{r}}^{(k)} + \widetilde{s}_{k}\widetilde{\vec{v}}^{(k)}
\step[\equiv]{unfolding tildes}
  \transpose{C}\vec{v}^{(k+1)}=C^{-1}\vec{r}^{(k)} + \widetilde{s}_{k}\transpose{C}\vec{v}^{(k)}
\step[\equiv]{folding in $\vec{w}^{(k)}$}
  \transpose{C}\vec{v}^{(k+1)}=\vec{w}^{(k)} + \widetilde{s}_{k}\transpose{C}\vec{v}^{(k)}
\step[\equiv]{multiply both sides by $(\transpose{C})^{-1}$ on the left}
  \vec{v}^{(k+1)}=(\transpose{C})^{-1}\vec{w}^{(k)} + \widetilde{s}_{k}\vec{v}^{(k)},
\end{calculation}
which gives us:
\begin{equation}
\boxed{\vec{v}^{(k+1)}=(\transpose{C})^{-1}\vec{w}^{(k)} + \widetilde{s}_{k}\vec{v}^{(k)}.}
\end{equation}
These boxed equations are precisely the conjugate-gradient method
applied to the conditioned system. Let us now collate these results in
one location:
\begin{align*}
\widetilde{s}_{k} &= \frac{\langle\vec{w}^{(k)},\vec{w}^{(k)}\rangle}{\langle\vec{w}^{(k-1)},\vec{w}^{(k-1)}\rangle}\\
\widetilde{t}_{k} &= \frac{\langle \vec{w}^{(k-1)}, \vec{w}^{(k-1)}\rangle}%
     {\langle\vec{v}^{(k)},A\widetilde{v}^{(k)}\rangle}\\
\vec{x}^{(k)} &= \vec{x}^{(k-1)} + \widetilde{t}_{k}\vec{v}^{(k)}\\
\vec{r}^{(k)} &= \vec{r}^{(k-1)} - \widetilde{t}_{k} A\vec{v}^{(k)}\\
\vec{w}^{(k)} &= C^{-1}\vec{r}^{(k)}\\
\vec{v}^{(k+1)} &= (\transpose{C})^{-1}\vec{w}^{(k)} + \widetilde{s}_{k}\vec{v}^{(k)}
\end{align*}
It's usually best to pick some $C$ which is easy to invert and take the
transpose of (hint: diagonal matrices).

\begin{algorithm}\label{alg:iterative-linear:conjugate-gradient}
  \caption{Preconditioned conjugate gradient method}
  \begin{algorithmic}[1]
    \Require $A=(a_{ij})$ is an $n\times n$ positive-definite matrix
    \Require $a_{ii}\neq0$ for each $i=1,\dots,n$
    \Require $C^{-1}=(c_{ij})$ is an $n\times n$ matrix
    \Require $\vec{b}$ is an $n$ vector
    \Require $\vec{x}^{(0)}$ is some initial guess $n$ vector
    \Ensure $\vec{x}$ is a solution
    \Function{ConjugateGradient}{$A$, $C^{-1}$, $\vec{b}$, $\vec{x}_{\text{init}}$, $\omega$, $\varepsilon_{\text{tol}}$, $N_{\text{max}}$}
    \State $\vec{x}\gets\vec{x}^{(0)}$ \algorithmiccomment{initialize}
    \State $\vec{r}\gets \vec{b}-A\vec{x}$
    \State $\vec{w}\gets C^{-1}\vec{r}$
    \State $\vec{v}\gets\transpose{(C^{-1})}\vec{w}$
    \State $\alpha\gets\sum^{n}_{j=1}w_{j}^{2}$

    \For{$k=1$, \dots, $N_{\text{max}}$}
      \If{$\|\vec{v}\|<\varepsilon_{\text{tol}}$}
        \State\Return $\vec{x}$
      \EndIf
      \State $\vec{u}\gets A\vec{v}$
      \State $t\gets \alpha/(\sum^{n}_{j=1}v_{j}u_{j})$
      \State $\vec{x}\gets\vec{x}+t\vec{u}$
      \State $\vec{r}\gets\vec{r}-t\vec{u}$
      \State $\vec{w}\gets C^{-1}\vec{r}$
      \State $\beta\gets\sum^{n}_{j=1}w_{j}^{2}$
      \If{$\beta<\varepsilon_{\text{tol}}$}
        \If{$\|\vec{r}\|<\varepsilon_{\text{tol}}$}
          \State\Return $\vec{x}$
        \EndIf
      \EndIf
      \State $s\gets\beta/\alpha$
      \State $\vec{v}\gets C^{-1}\vec{w}+s\vec{v}$
      \State $\alpha\gets\beta$
    \EndFor
    \State\Fail ``Maximum number of iterations exceeded''
  \EndFunction
\end{algorithmic}
\end{algorithm}

Note that when we are working with a huge system of equations, the
preconditioning matrix $C$ is approximately equal to $L$ in the Cholesky
factorization $L\transpose{L}=A$. Then
$\transpose{(C^{-1})}C^{-1}\approx A^{-1}$ (up to errors due to
floating-point arithmetic).

Also note that we would run into these situations (a large system of
equations, mostly sparse, but positive-definite) when trying to solve
boundary-value ordinary differential equations.

For more information about the conjugate gradient method,
Kelley~\cite{kelley1995} spends about the first third of his book
discussing it.

\begin{theorem}
The $\vec{x}^{(k)}$ approximation from the conjugate-gradient method
without preconditioning has its absolute error
$\vec{e}^{(k)}=\vec{x}^{(k)}-\vec{x}$ satisfy the bounds
\begin{equation}
\|\vec{e}^{(k)}\|_{A}\leq2\left(\frac{\sqrt{\condition(A)}-1}{\sqrt{\condition(A)}+1}\right)^{k}\|\vec{e}^{(0)}\|_{A}
\end{equation}
where $\|\vec{e}\|_{A}=\sqrt{\langle\vec{e},A\vec{e}\rangle}$ is the
energy norm associated with the positive-definite matrix $A$.
\end{theorem}

See, e.g., Hackbusch~\cite[\S10.2.3]{hackbusch2016iterative} for details.