\section{Power Method}

Assume $A$ is an $n\times n$ matrix, and suppose $\lambda_{1}$, \dots,
$\lambda_{n}$ are eigenvalues of $A$. Let $\vec{x}_{1}$, \dots,
$\vec{x}_{n}$ be the corresponding eigenvectors for $A$, and assume they
form a basis for $\CC^{n}$. Now assume that
\begin{equation*}
\abs{\lambda_{1}}>\abs{\lambda_{2}}\geq\abs{\lambda_{3}}\cdots\geq\abs{\lambda_{n}}\geq0
\end{equation*}
The power method will give us the \emph{dominant} eigenvalue
$\lambda_{1}$ and its associated eigenvector $\vec{x}_{1}$. How?

Well, the basic idea is to start with some initial guess
$\vec{z}^{(0)}$ for the eigenvector $\vec{x}_{1}$, then perform the
iteration
\begin{equation}
\vec{w}^{(m+1)}:=A\vec{z}^{(m)},\quad\mbox{and}\quad
\vec{z}^{(m+1)}:=\frac{\vec{w}^{(m+1)}}{\|\vec{w}^{(m+1)}\|}.
\end{equation}
The claim is that $\lim_{m\to\infty}\vec{z}^{(m+1)}=\vec{x}_{1}$. How
can we see this? Well, we can consider the first few iterates for
$\vec{z}^{(m)}$ and $\vec{w}^{(m)}$:
\begin{subequations}
\begin{align}
\vec{z}^{(0)} &= \mbox{guess}\\
\vec{w}^{(1)} &= A\vec{z}^{(0)}\\
\vec{z}^{(1)} &= \frac{\vec{w}^{(1)}}{\|\vec{w}^{(1)}\|} = \frac{A\vec{z}^{(0)}}{\|A\vec{z}^{(0)}\|}\\
\vec{w}^{(2)} &= A\vec{z}^{(1)}\\
\vec{z}^{(2)} &= \frac{\vec{w}^{(2)}}{\|\vec{w}^{(2)}\|}
= \left.\frac{A^{2}\vec{z}^{(0)}}{\|A\vec{z}^{(0)}\|} \middle/
\left\|\frac{A^{2}\vec{z}^{(0)}}{\|A\vec{z}^{(0)}\|}\right\|\right.
= \frac{A^{2}\vec{z}^{(0)}}{\|A^{2}\vec{z}^{(0)}\|}
\end{align}
We see that the $m^{\text{th}}$ iterate would give us
\begin{equation}
\vec{z}^{(m)} = \frac{A^{m}\vec{z}^{(0)}}{\|A^{m}\vec{z}^{(0)}\|}.
\end{equation}
\end{subequations}
Now why does this work? Let's neglect the normalization for the moment,
since it won't affect answering why the method works.
Expand $\vec{z}^{(0)}$ as a linear combination
of the eigenbasis $\vec{x}_{1}$, \dots, $\vec{x}_{n}$:
\begin{subequations}
\begin{align}
\vec{z}^{(0)} &= \alpha_{1}\vec{x}_{1}+\cdots+\alpha_{n}\vec{x}_{n}\\
\vec{z}^{(1)} &= A\vec{z}^{(0)}= \alpha_{1}\lambda_{1}\vec{x}_{1}+\cdots+\alpha_{n}\lambda_{n}\vec{x}_{n}\\
\vec{z}^{(2)} &= A^{2}\vec{z}^{(0)}= \alpha_{1}\lambda_{1}^{2}\vec{x}_{1}+\cdots+\alpha_{n}\lambda_{n}^{2}\vec{x}_{n}\\
\vec{z}^{(m)} &= A^{m}\vec{z}^{(0)}= \alpha_{1}\lambda_{1}^{m}\vec{x}_{1}+\cdots+\alpha_{n}\lambda_{n}^{m}\vec{x}_{n}
\end{align}
then for $\vec{z}^{(m)}$, we can factorize out $\lambda_{1}^{m}$
\begin{equation}
\vec{z}^{(m)} = \lambda_{1}^{m}\left(\alpha_{1}\vec{x}_{1}+\alpha_{2}\frac{\lambda_{2}^{m}}{\lambda_{1}^{m}}\vec{x}_{2}+\cdots+\alpha_{n}\frac{\lambda_{n}^{m}}{\lambda_{1}^{m}}\vec{x}_{n}\right),
\end{equation}
and since $\lambda_{1}$ is dominant, this is approximately equal to
\begin{equation}
\vec{z}^{(m)} \approx \lambda_{1}^{m}\alpha_{1}\vec{x}_{1}.
\end{equation}
Then we can obtain $\lambda_{1}$ and $\vec{x}_{1}$, as desired.
\end{subequations}

More explicitly, let $\lambda_{1}^{(k)}$ be the $k^{\text{th}}$
approximation of $\lambda_{1}$, let $\vec{x}^{(k)}$ be the associated
eigenvector. We begin with $\vec{x}^{(0)}$ such that
$\|\vec{x}^{(0)}\|_{\infty}=1$ (say, the vector consisting of all
components equal to 1). We have the temporary variable,
\begin{equation}
\vec{y} = A\vec{x}^{(k)}.
\end{equation}
We then find the smallest index $p$ such that
\begin{equation}
y_{p} = \|\vec{y}\|_{\infty}.
\end{equation}
Then we set
\begin{equation}
\lambda_{1}^{(k+1)}=y_{p},
\end{equation}
and
\begin{equation}
\vec{x}^{(k+1)} = \frac{\vec{y}}{y_{p}}.
\end{equation}
This is the iterative method.

\begin{algorithm}\label{alg:eigenvalues:power-method}
  \caption{Power method for matrix $A$}
  \begin{algorithmic}[1]
    \Require $A=(a_{ij})$ is an $n\times n$ matrix with dominant eigenvalue
    \Require $\vec{x}_{\text{init}}$ is some initial guess $n$ vector
    \Require $1>\varepsilon_{\text{tol}}>0$ is the error tolerance
    \Require $N_{\text{max}}>0$ is the maximum number of iterations
    \Ensure $\vec{x}$ is an eigenvector
    \Ensure $\lambda$ is the dominant eigenvalue of $A$
    \Function{PowerMethod}{$A$, $\vec{x}_{\text{init}}$, $\varepsilon_{\text{tol}}$, $N_{\text{max}}$}
    \State $\vec{x}\gets\vec{x}_{\text{init}}$ \algorithmiccomment{initialize}
    \State Find the smallest $p\in\NN$ such that $p\leq n$ and $|x_{p}|=\|\vec{x}\|_{\infty}$
    \State $\vec{x}\gets \vec{x}_{\text{init}}/x_{p}$
    \For{$k=1$, \dots, $N_{\text{max}}$}
      \State $\vec{y}\gets A\vec{x}$
      \State Find the smallest $p\in\NN$ such that $p\leq n$ and $|y_{p}|=\|\vec{y}\|_{\infty}$
      \State $\lambda\gets y_{p}$
      \If{$\lambda = 0$}
        \State\Fail ``Matrix $A$ has a zero eigenvalue, guess a different $x_{\text{init}}$''
      \EndIf
      \State $r\gets \|\vec{x}-\vec{y}/\lambda\|_{\infty}$
      \State $\vec{x}\gets\vec{y}/\lambda$
      \If{$|r|<\varepsilon_{\text{tol}}$}
        \State\Return $(\lambda,\vec{x})$
      \EndIf
    \EndFor
    \State\Fail ``Maximum number of iterations exceeded''
  \EndFunction
\end{algorithmic}
\end{algorithm}
