\section{Newton's Method}

Suppose we want to find the root of some $f\in C^{1}[a,b]$ which has at
least continuous first-derivatives. We can take the linear approximation
\begin{equation}
  f(p) \approx f(p_{0}) + f'(p_{0})(p - p_{0}).
\end{equation}
If $p$ were a root, we would have
\begin{equation}
  f(p) = 0\implies p = p_{0} - \frac{f(p_{0})}{f'(p_{0})}.
\end{equation}
This gives us an iterative algorithm
\begin{equation}
p_{n+1} = p_{n} - \frac{f(p_{n})}{f'(p_{n})}
\end{equation}
which we claim converges to a root $p_{n}\to p$.

\begin{algorithm}\label{alg:root:newton-method}
  \caption{Newton's Method for finding roots}
  \begin{algorithmic}[1]
    \Require $f$ is continuously differentiable on the interval $[\ell,u]$
    \Require $f'$ is continuous and the derivative of $f$
    \Require an initial guess $p_{0}$
    \Require $0<\varepsilon_{\text{tol}}\ll 1$
    \Require $N_{\text{max}}\in\NN$
    \Ensure $f(\mbox{result}) < \varepsilon_{\text{tol}}$ or failure
    \Function{Newton}{$f$, $f'$, $p_{0}$,$N_{\text{max}}$, $\varepsilon_{\text{tol}}$}
    \State Initialize $n\gets 0$, $p_{\text{prev}}\gets p_{0}$
    \For{$n=1,N_{\text{max}}$}
      \State $p\gets p_{\text{prev}} - f(p_{\text{prev}})/f'(p_{\text{prev}})$
      \If{$|p-p_{\text{prev}}| < \varepsilon_{\text{tol}}$}
      \State\Return $p$
      \EndIf
      \State $p_{\text{prev}}\gets p$
    \EndFor
    \State\Fail ``Error: failed after $N_{\text{max}}$ iterations''
  \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{thm}[Convergence]
Let $f\in C^{2}[a,b]$ be such that if $a\leq p\leq b$ is a root $f(p)=0$
with nonzero derivative $f'(p)\neq0$,
then there exists a $\delta>0$ such that for any $p_{0}\in[p-\delta,p+\delta]$
the resulting sequence $\{p_{n}\}_{n\in\NN_{0}}$ generated by Newton's
method converges to $p$ (i.e., $p_{n}\to p$ as $n\to\infty$).
\end{thm}

The proof consists of two steps, then applying the fixed-point Theorem~\ref{thm:root:fixed-point:fixed-point-thm}.

\begin{proof}
  Let
  \begin{equation}
    g(x) = x - \frac{f(x)}{f'(x)}.
  \end{equation}
  We construct an interval $[p-\delta, p+\delta]$ such that any $x$ in
  the $\delta$-neighborhood of $p$ satisfies
  \begin{equation}
    |g'(x)|\leq k
  \end{equation}
  for some $0<k<1$. Then $g$ maps this $\delta$-neighborhood of $p$ to
  itself.
  Hence by the Fixed-Point Theorem~\ref{thm:root:fixed-point:fixed-point-thm},
  the sequence $\{p_{n}\}_{n\in\NN_{0}}$ defined by $p_{n+1}=g(p_{n})$
  converges to $p$ for any $p_{0}\in[p-\delta,p+\delta]$.

  But how do we construct the interval $[p-\delta, p+\delta]$?
  We have $g$ defined on $[p-\delta_{1},p+\delta_{1}]$. Then
  \begin{equation}
    g'(x) = \frac{f(x)f''(x)}{[f'(x)]^{2}}.
  \end{equation}
  Let $k$ be such that $0<k<1$. There is a corresponding $\delta>0$ such
  that $\delta<\delta_{1}$ and $|g'(x)|\leq k$ for each $x\in[p-\delta,p+\delta]$.
  We want to show $g$ maps this interval to itself.

  Now, to prove $g$ maps the interval $[p-\delta,p+\delta]$ to itself,
  we use the mean-value theorem which implies there exists some
  $\xi\in[p-\delta,p+\delta]$ such that
  \begin{equation}
    |g(x)-g(p)| = |g'(\xi)|\cdot|x-p|
  \end{equation}
  for $x\in[p-\delta,p+\delta]$. Since $g(p)=p$, we find
  \begin{subequations}
    \begin{align}
      |g(x) - g(p)|
      &= |g(x) - p|\\
      &= |g'(\xi)|\cdot|x-p|\\
      &\leq k\cdot|x-p| < |x-p|
    \end{align}
  \end{subequations}
  since $|g'(\xi)|\leq k<1$. Hence $|g(x)-p|<\delta$ implies $g(x)$ maps
  the interval to itself.
\end{proof}

\begin{thm}[Quadratic Convergence]
Let $f\in C^{2}(a,b)$, $p\in(a,b)$ be a root of $f$, $f'(p)\neq0$, and
let $\{p_{k}\}$ be a sequence generated by Newton's method.

If $p_{k}\to p$ converges, then for $k$ sufficiently large
\begin{equation}
  |p_{k+1}-p_{k}|\leq M|p_{k}-p|^{2}
\end{equation}
provided
\begin{equation}
  M > \frac{|f''(p)|}{2|f'(p)|}.
\end{equation}
In other words, Newton's method converges quadratically.
\end{thm}
\begin{proof}
Let $\varepsilon_{k}=p_{k}-p$, so $p=p_{k}-\varepsilon_{k}$. We will
show $\varepsilon_{k+1}\sim\varepsilon_{k}^{2}$.

We Taylor expand $f(p)$ about $p_{k}$:
\begin{equation}
  f(p_{k}-\varepsilon_{k}) = f(p_{k}) - \varepsilon_{k}f'(p_{k}) + \frac{1}{2}\varepsilon_{k}^{2}f''(\xi_{k})
\end{equation}
for some $\xi_{k}$ between $p_{k}$ and $p$. Since $f(p)=f(p_{k}-\varepsilon_{k})=0$,
we have
\begin{equation}
  0 = f(p_{k}) - \varepsilon_{k}f'(p_{k}) + \frac{1}{2}\varepsilon_{k}^{2}f''(\xi_{k}).
\end{equation}
Since $f'(p_{k})\neq0$,
\begin{equation}
  0 = \underbrace{\frac{f(p_{k})}{f'(p_{k})}-p_{k}}_{=-p_{k+1}} + p +
  \frac{1}{2}\varepsilon_{k}^{2}\frac{f''(\xi_{k})}{f'(p_{k})}
\end{equation}
but this can be rewritten as
\begin{equation}
  0 = -\varepsilon_{k+1}
  +\frac{1}{2}\varepsilon_{k}^{2}\frac{f''(\xi_{k})}{f'(p_{k})}
\end{equation}
Simply add $\varepsilon_{k+1}$ to both sides, then take the absolute
value of both sides to obtain the desired result.
\end{proof}


\begin{chunk}
  The \FORTRAN/ implementation of this requires some care. The
  \verb#newton# function returns whether the method succeeded or
  failed; if it succeeds, then it will set the \verb#result# parameter
  to the approximate fixed-point.\marginpar{\texttt{root.f90}}
  \lstinputlisting[language=fortran18,firstline=67,lastline=89]{src/root.f90}
\end{chunk}
