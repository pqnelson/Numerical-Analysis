\section{Divided Differences}

The basic idea is we're given $n+1$ observations $(x_{0},y_{0})$,
$(x_{1},y_{1})$, \dots, $(x_{n}, y_{n})$ with $x_{i}\neq x_{j}$ for
$i\neq j$. We suppose $x_{0} < x_{1} < \dots < x_{n}$. What to do? We
can construct a table:
\begin{center}
  \begin{tabular}{c|c}
    $x_{0}$ & $f[x_{0}]=y_{0}$\\
    $x_{1}$ & $f[x_{1}]=y_{1}$\\
    \vdots & \vdots\\
    $x_{n}$ & $f[x_{n}]=y_{n}$
  \end{tabular}
\end{center}
The strategy is to construct a polynomial
\begin{equation}
  P(x) = c_{0} + c_{1}(x - x_{0}) + c_{2}(x - x_{0})(x - x_{1}) + \dots
  + c_{n}(x - x_{0})(x - x_{1})(\dots)(x - x_{n})
\end{equation}
which can be applied to Horner's method. The only problem: determine the
unknown coefficients $c_{0}$, $c_{1}$, \dots, $c_{n}$.

We begin by observing
\begin{equation}
  P(x_{0}) = c_{0} = y_{0}.
\end{equation}
Then for the linear term
\begin{equation}
  P(x_{1}) = y_{0} + c_{1}(x_{1}-x_{0}) = y_{1}.
\end{equation}
We can rearrange terms to find
\begin{equation}
  c_{1} = \frac{y_{1} - y_{0}}{x_{1} - x_{0}}.
\end{equation}
Now examining the quadratic term:
\begin{equation}
  P(x_{2}) = y_{0} + \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{0})
  + c_{2}(x_{2} - x_{0})(x_{2} - x_{1}) = y_{2}.
\end{equation}
We can subtract $y_{0}$ from both sides, and some algebra gives
\begin{subequations}
\begin{equation}
  \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{1} + x_{1} - x_{0})
  + c_{2}(x_{2} - x_{1})(x_{2} - x_{0}) = y_{2} - y_{0},
\end{equation}
which simplifies to
\begin{equation}
  \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{1}) + (y_{1} - y_{0})
  + c_{2}(x_{2} - x_{1})(x_{2} - x_{0}) = y_{2} - y_{0}.
\end{equation}
Subtracting $(y_{1}-y_{0})$ from both sides, then factoring the
left-hand side
\begin{equation}
  \left(\frac{y_{1} - y_{0}}{x_{1} - x_{0}}
  + c_{2}(x_{2} - x_{0})\right)(x_{2} - x_{1}) = y_{2} - y_{1},
\end{equation}
Dividing both sides by $x_{2}-x_{1}$, then subtraction gives
\begin{equation}
  c_{2}(x_{2} - x_{0}) = \frac{y_{2}-y_{1}}{x_{2}-x_{1}} - \frac{y_{1} - y_{0}}{x_{1} - x_{0}}.
\end{equation}
\end{subequations}
Dividing through by $x_{2}-x_{0}$ gives us
\begin{equation}
  c_{2} = \frac{\displaystyle\frac{y_{2}-y_{1}}{x_{2}-x_{1}} - \frac{y_{1} - y_{0}}{x_{1} - x_{0}}}{x_{2} - x_{0}}.
\end{equation}
This motivates the following notions of divided differences:
\begin{enumerate}
\item The zeroth differences $f[x_{j}] = y_{j}$
\item The first differences $f[x_{j},x_{j+1}] = (f[x_{j+1}]-f[x_{j}])/(x_{j+1}-x_{j})$
\item The second differences $f[x_{j},x_{j+1},x_{j+2}] = (f[x_{j+1},x_{j+2}]-f[x_{j},x_{j+1}])/(x_{j+2}-x_{j})$
\item The $k^{\text{th}}$ differences
  $f[x_{j},x_{j+1},x_{j+2},\dots,x_{j+k}] = \displaystyle\frac{(f[x_{j+1},x_{j+2},\dots,x_{j+k}]-f[x_{j},x_{j+1},\dots,x_{j+k-1}])}{x_{j+k}-x_{j}}$
\end{enumerate}
Then Newton's divided difference method gives us the interpolating polynomial:
\begin{equation}
  \begin{split}
  P(x) &= f[x_{0}] + f[x_{0},x_{1}](x - x_{0}) + \dots +
  f[x_{0},\dots,x_{k}](x-x_{0})(\dots)(x-x_{k-1})+\dots\\
  &\qquad+f[x_{0},\dots,x_{n}](x-x_{0})(\dots)(x-x_{n-1}).
  \end{split}
\end{equation}
Really, this is for \emph{forward differences}. The generic divided
differences method constructs the table:
\begin{center}
  \begin{tabular}{c|ccccc}
    $x_{0}$ & $f[x_{0}]=y_{0}$& & & & \\
           &                 & $f[x_{0},x_{1}] = \frac{f[x_{1}]-f[x_{0}]}{x_{1}-x_{0}}$& & & \\
    $x_{1}$ & $f[x_{1}]=y_{1}$ &    & $f[x_{0},x_{1},x_{2}]$ &  & \\
           &    & $f[x_{1},x_{2}]$& & $\ddots$ & \\
    \vdots & \vdots & \vdots & \vdots & $\dots$ &  $f[x_{0},x_{1},\dots,x_{n}]$ \\
           &    & $f[x_{n-1},x_{n-2}]$& & \reflectbox{$\ddots$} & \\
    $x_{n-1}$ & $f[x_{n-1}]=y_{n-1}$ &   & $f[x_{n-2},x_{n-1},x_{n}]$ &  &\\
           &                 & $f[x_{n-1},x_{n}] = \frac{f[x_{n}]-f[x_{n-1}]}{x_{n}-x_{n-1}}$& & & \\
    $x_{n}$ & $f[x_{n}]=y_{n}$ && 
  \end{tabular}
\end{center}
Then we take a ``path'' in this table, using the coefficients and
factors accordingly. If we move ``up'' in our path, it costs us a sign.
This is rather abstract, let's look at a couple of examples.

First, we can recover Newton's divided differences from this table by
moving along the top of the table.

We could have moved along the ``bottom path'' instead, taking
\begin{equation}
  \begin{split}
  \widetilde{P}(x) &= f[x_{n}] - f[x_{n-1},x_{n}](x - x_{n})
  - f[x_{n-2},x_{n-1},x_{n}](x - x_{n-1})(x - x_{n}) - \dots \\
 &\qquad - f[x_{0}, x_{1}, \dots, x_{n}](x - x_{0})(x - x_{1})(\dots)(x - x_{n}).
  \end{split}
\end{equation}


\begin{algorithm}\label{alg:divided-diff:newton}
  \caption{Newton's Divided-Differences Method for polynomial interpolation}
  \begin{algorithmic}[1]
    \Require $(x_{0},y_{0})$, \dots, $(x_{n},y_{n})$ are $n+1$ distinct observations
    \Ensure the result are numbers $F_{0,0}$, $F_{1,1}$, \dots,
    $F_{n,n}$ where $P(x) = \sum_{j=0}^{n} F_{j,j}\prod^{j-1}_{k=0}(x-x_{k})$
    \Function{NewtonInterpolation}{$x$, $y$}
    \State Initialize $F_{0,\cdot} \gets y_{\cdot}$
    \For{$i=1,n$}
      \For{$j=1,i$}
        \State Set $F_{i,j}\gets\frac{F_{i,j-1} - F_{i-1,j-1}}{x_{i}-x_{i-j}}$
      \EndFor
    \EndFor
    \State\Return $(F_{0,0}, F_{1,1}, \dots, F_{n,n})$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{ex}
  Find the Newton divided-differences polynomial using the data from Example~\ref{ex:interpolation:lagrange:random-example}: $(1,0)$,
  $(9,12)$, $(3,6)$.

  We can setup the table:
  \begin{center}
    \begin{tabular}{c|ccc}
      $x_{0}=1$ & $f[x_{0}]=0$  &                 & \\
                &              & $f[x_{0},x_{1}]=6/2=3$ & \\
      $x_{1}=3$  & $f[x_{1}]=6$ &      & $f[x_{0},x_{1},x_{2}]=(1-3)/(9-1)=-1/4$\\
                &              & $f[x_{1},x_{2}]=6/6=1$ & \\
      $x_{2}=9$ & $f[x_{2}]=12$  &                 & 
    \end{tabular}
  \end{center}
  Then we have
  \begin{equation}
    P(x) = 0 + 3(x - 1) - \frac{1}{4}(x - 1)(x - 3).
  \end{equation}
  Does this work? Well, Lagrange polynomial interpolation gave us:
  \begin{equation}
    Q(x) = \frac{-1}{4}x^{2} + 4x - \frac{15}{4}
  \end{equation}
  whereas Newton gives us
  \begin{equation}
    P(x) % = 3x - 3 - \frac{1}{4}(x^{2} - 4x + 3)
    = \frac{-1}{4}x^{2} + 4x - \frac{15}{4}.
  \end{equation}
  The two methods agree!
\end{ex}

\begin{ex}
Let's try to approximate $\sin(x)$ using the observations $(0,0)$,
$(\pi/6,1/2)$, $(\pi/4,\sqrt{2}/2)$, $(\pi/3,\sqrt{3}/2)$, $(\pi/2,1)$.
We can compare the result to Example~\ref{ex:interpolation:lagrange:sine},
when we used Lagrange interpolation.

We have
\begin{center}
\begin{tabular}{c|ccc}
  $x_{0}=0$ & $f[x_{0}]=0$ & &  \\
  
           & & $f[x_{0},x_{1}] = \displaystyle\frac{3}{\pi}$ & \\
  
  $x_{1}=\displaystyle\frac{\pi}{6}$ & $f[x_{1}]=\displaystyle\frac{1}{2}$ & & $f[x_{1},x_{2},x_{3}]=\displaystyle\frac{12}{\pi^{2}}(\sqrt{2}-3)$  \\
  
            & & $f[x_{1},x_{2}] =\displaystyle \frac{6}{\pi}(\sqrt{2}-1)$ & \\

  $x_{2}=\displaystyle\frac{\pi}{4}$ & $f[x_{2}]=\displaystyle\frac{\sqrt{2}}{2}$ & & $f[x_{1},x_{2},x_{3}]=\displaystyle\frac{36}{\pi^{2}}(1+\sqrt{3}-2\sqrt{2})$ \\
  
            & & $f[x_{2},x_{3}] = \displaystyle\frac{6}{\pi}(\sqrt{3} - \sqrt{2})$ & \\

  $x_{3}=\displaystyle\frac{\pi}{3}$ & $f[x_{3}]=\displaystyle\frac{\sqrt{3}}{2}$ & & $f[x_{2},x_{3},x_{4}]=\displaystyle\frac{12}{\pi^{2}}(2+2\sqrt{2}-3\sqrt{3})$ \\
  
            & & $f[x_{3},x_{4}] = \displaystyle\frac{3}{\pi}(2 - \sqrt{3})$ &\\

  $x_{4}=\displaystyle\frac{\pi}{2}$ & $f[x_{4}]=1$ & & 
\end{tabular}
\end{center}
And the higher differences:
\begin{center}
  \begin{tabular}{cc}
    $f[x_{0},x_{1},x_{2},x_{3}]=\displaystyle\frac{36}{\pi^{3}}(6+3\sqrt{3}-8\sqrt{2})$&\\
    &$f[x_{0},x_{1},x_{2},x_{3},x_{4}]=\displaystyle\frac{72}{\pi^{4}}(16\sqrt{2}-7-9\sqrt{3})$\\
    $f[x_{1},x_{2},x_{3},x_{4}]=\displaystyle\frac{36}{\pi^{3}}(8\sqrt{2}-1-3\sqrt{3})$&
  \end{tabular}
\end{center}
The Newton's divided-difference polynomial gives us
\begin{equation}
  \begin{split}
  P(x) &= \frac{3}{\pi}x + \frac{12}{\pi^{2}}(\sqrt{2}-3)x(x-\pi/6)\\
  &\qquad+ \frac{36}{\pi^{3}}(6+3\sqrt{3}-8\sqrt{2}) x(x-\pi/6)(x - \pi/4)\\
  &\qquad+ \frac{72}{\pi^{4}}(16\sqrt{2}-7-9\sqrt{3})x(x-\pi/6)(x - \pi/4)(x-\pi/2)
  \end{split}
\end{equation}
We can simplify, a bit, to get
%% (25/Pi - (32*Sqrt[2])/Pi + (27*Sqrt[3])/(2*Pi))*x + 
%%  (-217/Pi^2 + (352*Sqrt[2])/Pi^2 - (162*Sqrt[3])/Pi^2)*x^2 + 
%%  (594/Pi^3 - (1152*Sqrt[2])/Pi^3 + (594*Sqrt[3])/Pi^3)*x^3 + 
%%  (-504/Pi^4 + (1152*Sqrt[2])/Pi^4 - (648*Sqrt[3])/Pi^4)*x^4
\begin{equation}
  \begin{split}
    P(x) &= (50-64 \sqrt{2}+27 \sqrt{3})\frac{x}{2\pi}\\
&\quad-[217-352 \sqrt{2}+162 \sqrt{3}]\frac{x^{2}}{\pi^{2}}\\
&\quad-18[33-64 \sqrt{2}+33 \sqrt{3}]\frac{x^{3}}{\pi^{3}}\\
&\quad-72[7-16 \sqrt{2}+9 \sqrt{3}]\frac{x^{4}}{\pi^{4}}
  \end{split}
\end{equation}
Or, as a single-precision approximation to this polynomial:
\begin{equation}
  P(x)\approx 0.99562618 x +0.021373008 x^2-0.20434070 x^3+ 0.028797112 x^4
\end{equation}
What does Lagrange polynomial interpolation give us? Well, we can recall Eq~\eqref{eq:interpolation:polynomial:sine}
\begin{equation*}
  \begin{split}
    P_{\text{Lagrange}}(x)
      &=
      [50-64 \sqrt{2}+27 \sqrt{3}]\frac{x}{2\pi}
      -[217-352 \sqrt{2}+162 \sqrt{3}]\frac{x^{2}}{\pi^{2}}\\
      &\quad- 18 [33-64 \sqrt{2}+33 \sqrt{3}]\frac{x^{3}}{\pi^{3}}
      - 72 (7 - 16\sqrt{2} + 9\sqrt{3})\frac{x^{4}}{\pi^{4}}
  \end{split}
\end{equation*}
which is clearly the same polynomial.

Why have two different methods for polynomial interpolation? Well, if we
keep the table of divided differences, adding a datapoint requires only
$2(n-2)$ subtraction operations and $n-2$ division operations. But for
Lagrange interpolation, we have to start over from scratch. There's no
easy way to update an existing Lagrange polynomial interpolation.
\end{ex}
