\section{Divided Differences}

The basic idea is we're given $n+1$ observations $(x_{0},y_{0})$,
$(x_{1},y_{1})$, \dots, $(x_{n}, y_{n})$ with $x_{i}\neq x_{j}$ for
$i\neq j$. We suppose $x_{0} < x_{1} < \dots < x_{n}$. What to do? We
can construct a table:
\begin{center}
  \begin{tabular}{c|c}
    $x_{0}$ & $f[x_{0}]=y_{0}$\\
    $x_{1}$ & $f[x_{1}]=y_{1}$\\
    \vdots & \vdots\\
    $x_{n}$ & $f[x_{n}]=y_{n}$
  \end{tabular}
\end{center}
The strategy is to construct a polynomial
\begin{equation}
  P(x) = c_{0} + c_{1}(x - x_{0}) + c_{2}(x - x_{0})(x - x_{1}) + \dots
  + c_{n}(x - x_{0})(x - x_{1})(\dots)(x - x_{n})
\end{equation}
which can be applied to Horner's method. The only problem: determine the
unknown coefficients $c_{0}$, $c_{1}$, \dots, $c_{n}$.

We begin by observing
\begin{equation}
  P(x_{0}) = c_{0} = y_{0}.
\end{equation}
Then for the linear term
\begin{equation}
  P(x_{1}) = y_{0} + c_{1}(x_{1}-x_{0}) = y_{1}.
\end{equation}
We can rearrange terms to find
\begin{equation}
  c_{1} = \frac{y_{1} - y_{0}}{x_{1} - x_{0}}.
\end{equation}
Now examining the quadratic term:
\begin{equation}
  P(x_{2}) = y_{0} + \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{0})
  + c_{2}(x_{2} - x_{0})(x_{2} - x_{1}) = y_{2}.
\end{equation}
We can subtract $y_{0}$ from both sides, and some algebra gives
\begin{subequations}
\begin{equation}
  \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{1} + x_{1} - x_{0})
  + c_{2}(x_{2} - x_{1})(x_{2} - x_{0}) = y_{2} - y_{0},
\end{equation}
which simplifies to
\begin{equation}
  \frac{y_{1} - y_{0}}{x_{1} - x_{0}}(x_{2} - x_{1}) + (y_{1} - y_{0})
  + c_{2}(x_{2} - x_{1})(x_{2} - x_{0}) = y_{2} - y_{0}.
\end{equation}
Subtracting $(y_{1}-y_{0})$ from both sides, then factoring the
left-hand side
\begin{equation}
  \left(\frac{y_{1} - y_{0}}{x_{1} - x_{0}}
  + c_{2}(x_{2} - x_{0})\right)(x_{2} - x_{1}) = y_{2} - y_{1},
\end{equation}
Dividing both sides by $x_{2}-x_{1}$, then subtraction gives
\begin{equation}
  c_{2}(x_{2} - x_{0}) = \frac{y_{2}-y_{1}}{x_{2}-x_{1}} - \frac{y_{1} - y_{0}}{x_{1} - x_{0}}.
\end{equation}
\end{subequations}
Dividing through by $x_{2}-x_{0}$ gives us
\begin{equation}
  c_{2} = \frac{\displaystyle\frac{y_{2}-y_{1}}{x_{2}-x_{1}} - \frac{y_{1} - y_{0}}{x_{1} - x_{0}}}{x_{2} - x_{0}}.
\end{equation}
This motivates the following notions of divided differences:
\begin{enumerate}
\item The zeroth differences $f[x_{j}] = y_{j}$
\item The first differences $f[x_{j},x_{j+1}] = (f[x_{j+1}]-f[x_{j}])/(x_{j+1}-x_{j})$
\item The second differences $f[x_{j},x_{j+1},x_{j+2}] = (f[x_{j+1},x_{j+2}]-f[x_{j},x_{j+1}])/(x_{j+2}-x_{j})$
\item The $k^{\text{th}}$ differences
  $f[x_{j},x_{j+1},x_{j+2},\dots,x_{j+k}] = \displaystyle\frac{(f[x_{j+1},x_{j+2},\dots,x_{j+k}]-f[x_{j},x_{j+1},\dots,x_{j+k-1}])}{x_{j+k}-x_{j}}$
\end{enumerate}
Then Newton's divided difference method gives us the interpolating polynomial:
\begin{equation}
  \begin{split}
  P(x) &= f[x_{0}] + f[x_{0},x_{1}](x - x_{0}) + \dots +
  f[x_{0},\dots,x_{k}](x-x_{0})(\dots)(x-x_{k-1})+\dots\\
  &\qquad+f[x_{0},\dots,x_{n}](x-x_{0})(\dots)(x-x_{n-1}).
  \end{split}
\end{equation}
Really, this is for \emph{forward differences}. The generic divided
differences method constructs the table:
\begin{center}
  \begin{tabular}{c|ccccc}
    $x_{0}$ & $f[x_{0}]=y_{0}$& & & & \\
           &                 & $f[x_{0},x_{1}] = \frac{f[x_{1}]-f[x_{0}]}{x_{1}-x_{0}}$& & & \\
    $x_{1}$ & $f[x_{1}]=y_{1}$ &    & $f[x_{0},x_{1},x_{2}]$ &  & \\
           &    & $f[x_{1},x_{2}]$& & $\ddots$ & \\
    \vdots & \vdots & \vdots & \vdots & $\dots$ &  $f[x_{0},x_{1},\dots,x_{n}]$ \\
           &    & $f[x_{n-1},x_{n-2}]$& & \reflectbox{$\ddots$} & \\
    $x_{n-1}$ & $f[x_{n-1}]=y_{n-1}$ &   & $f[x_{n-2},x_{n-1},x_{n}]$ &  &\\
           &                 & $f[x_{n-1},x_{n}] = \frac{f[x_{n}]-f[x_{n-1}]}{x_{n}-x_{n-1}}$& & & \\
    $x_{n}$ & $f[x_{n}]=y_{n}$ && 
  \end{tabular}
\end{center}
Then we take a ``path'' in this table, using the coefficients and
factors accordingly. If we move ``up'' in our path, it costs us a sign.
This is rather abstract, let's look at a couple of examples.

First, we can recover Newton's divided differences from this table by
moving along the top of the table.

We could have moved along the ``bottom path'' instead, taking
\begin{equation}
  \begin{split}
  \widetilde{P}(x) &= f[x_{n}] - f[x_{n-1},x_{n}](x - x_{n})
  - f[x_{n-2},x_{n-1},x_{n}](x - x_{n-1})(x - x_{n}) - \dots \\
 &\qquad - f[x_{0}, x_{1}, \dots, x_{n}](x - x_{0})(x - x_{1})(\dots)(x - x_{n}).
  \end{split}
\end{equation}


\begin{algorithm}\label{alg:divided-diff:newton}
  \caption{Newton's Divided-Differences Method for polynomial interpolation}
  \begin{algorithmic}[1]
    \Require $(x_{0},y_{0})$, \dots, $(x_{n},y_{n})$ are $n+1$ distinct observations
    \Ensure the result are numbers $F_{0,0}$, $F_{1,1}$, \dots,
    $F_{n,n}$ where $P(x) = \sum_{j=0}^{n} F_{j,j}\prod^{j-1}_{k=0}(x-x_{k})$
    \Function{NewtonInterpolation}{$x$, $y$}
    \State Initialize $F_{0,\cdot} \gets y_{\cdot}$
    \For{$i=1,n$}
      \For{$j=1,i$}
        \State Set $F_{i,j}\gets\frac{F_{i,j-1} - F_{i-1,j-1}}{x_{i}-x_{i-j}}$
      \EndFor
    \EndFor
    \State\Return $(F_{0,0}, F_{1,1}, \dots, F_{n,n})$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{ex}
  Find the Newton divided-differences polynomial using the data from Example~\ref{ex:interpolation:lagrange:random-example}: $(1,0)$,
  $(9,12)$, $(3,6)$.

  We can setup the table:
  \begin{center}
    \begin{tabular}{c|ccc}
      $x_{0}=1$ & $f[x_{0}]=0$  &                 & \\
                &              & $f[x_{0},x_{1}]=6/2=3$ & \\
      $x_{1}=3$  & $f[x_{1}]=6$ &      & $f[x_{0},x_{1},x_{2}]=(1-3)/(9-1)=-1/4$\\
                &              & $f[x_{1},x_{2}]=6/6=1$ & \\
      $x_{2}=9$ & $f[x_{2}]=12$  &                 & 
    \end{tabular}
  \end{center}
  Then we have
  \begin{equation}
    P(x) = 0 + 3(x - 1) - \frac{1}{4}(x - 1)(x - 3).
  \end{equation}
  Does this work? Well, Lagrange polynomial interpolation gave us:
  \begin{equation}
    Q(x) = \frac{-1}{4}x^{2} + 4x - \frac{15}{4}
  \end{equation}
  whereas Newton gives us
  \begin{equation}
    P(x) % = 3x - 3 - \frac{1}{4}(x^{2} - 4x + 3)
    = \frac{-1}{4}x^{2} + 4x - \frac{15}{4}.
  \end{equation}
  The two methods agree!
\end{ex}

\begin{ex}
Let's try to approximate $\sin(x)$ using the observations $(0,0)$,
$(\pi/6,1/2)$, $(\pi/4,\sqrt{2}/2)$, $(\pi/3,\sqrt{3}/2)$, $(\pi/2,1)$.
We can compare the result to Example~\ref{ex:interpolation:lagrange:sine},
when we used Lagrange interpolation.

We have
\begin{center}
\begin{tabular}{c|ccc}
  $x_{0}=0$ & $f[x_{0}]=0$ & &  \\
  
           & & $f[x_{0},x_{1}] = \displaystyle\frac{3}{\pi}$ & \\
  
  $x_{1}=\displaystyle\frac{\pi}{6}$ & $f[x_{1}]=\displaystyle\frac{1}{2}$ & & $f[x_{1},x_{2},x_{3}]=\displaystyle\frac{12}{\pi^{2}}(\sqrt{2}-3)$  \\
  
            & & $f[x_{1},x_{2}] =\displaystyle \frac{6}{\pi}(\sqrt{2}-1)$ & \\

  $x_{2}=\displaystyle\frac{\pi}{4}$ & $f[x_{2}]=\displaystyle\frac{\sqrt{2}}{2}$ & & $f[x_{1},x_{2},x_{3}]=\displaystyle\frac{36}{\pi^{2}}(1+\sqrt{3}-2\sqrt{2})$ \\
  
            & & $f[x_{2},x_{3}] = \displaystyle\frac{6}{\pi}(\sqrt{3} - \sqrt{2})$ & \\

  $x_{3}=\displaystyle\frac{\pi}{3}$ & $f[x_{3}]=\displaystyle\frac{\sqrt{3}}{2}$ & & $f[x_{2},x_{3},x_{4}]=\displaystyle\frac{12}{\pi^{2}}(2+2\sqrt{2}-3\sqrt{3})$ \\
  
            & & $f[x_{3},x_{4}] = \displaystyle\frac{3}{\pi}(2 - \sqrt{3})$ &\\

  $x_{4}=\displaystyle\frac{\pi}{2}$ & $f[x_{4}]=1$ & & 
\end{tabular}
\end{center}
And the higher differences:
\begin{center}
  \begin{tabular}{cc}
    $f[x_{0},x_{1},x_{2},x_{3}]=\displaystyle\frac{36}{\pi^{3}}(6+3\sqrt{3}-8\sqrt{2})$&\\
    &$f[x_{0},x_{1},x_{2},x_{3},x_{4}]=\displaystyle\frac{72}{\pi^{4}}(16\sqrt{2}-7-9\sqrt{3})$\\
    $f[x_{1},x_{2},x_{3},x_{4}]=\displaystyle\frac{36}{\pi^{3}}(8\sqrt{2}-1-3\sqrt{3})$&
  \end{tabular}
\end{center}
The Newton's divided-difference polynomial gives us
\begin{equation}
  \begin{split}
  P(x) &= \frac{3}{\pi}x + \frac{12}{\pi^{2}}(\sqrt{2}-3)x(x-\pi/6)\\
  &\qquad+ \frac{36}{\pi^{3}}(6+3\sqrt{3}-8\sqrt{2}) x(x-\pi/6)(x - \pi/4)\\
  &\qquad+ \frac{72}{\pi^{4}}(16\sqrt{2}-7-9\sqrt{3})x(x-\pi/6)(x - \pi/4)(x-\pi/2)
  \end{split}
\end{equation}
We can simplify, a bit, to get
\begin{equation}
  \begin{split}
P(x) &= [36 - 32\sqrt{2} + 9\sqrt{3} - (7 - 16\sqrt{2} + 9\sqrt{3})\pi]\frac{x}{2\pi}\\
&\quad-\frac{3}{2\pi^{2}}[84 - 96\sqrt{2} + 30\sqrt{3} - 5\pi(7 - 16\sqrt{2}+9\sqrt{3})]x^{2}\\
&\quad+ \frac{4}{\pi^{3}}[9(6 - 8\sqrt{2} + 3\sqrt{3}) - 10\pi(7 - 16\sqrt{2}
  + 9\sqrt{3})]x^{3}\\
&\quad+ \frac{90}{\pi^{3}}(7 - 16\sqrt{2} + 9\sqrt{3}) x^{4}\\
&\quad- \frac{72}{\pi^{4}}(7 - 16\sqrt{2} + 9\sqrt{3}) x^{5}
  \end{split}
\end{equation}
Or, as a single-precision approximation to this polynomial:
\begin{equation}
  P(x)\approx 1.0275073 x - 0.12295329 x^{2}  + 0.021408739 x^{3}  - 0.11308600 x^{4}
+ 0.028797112 x^{5}
\end{equation}
What does Lagrange polynomial interpolation give us? Well, we can recall Eq~\eqref{eq:interpolation:polynomial:sine}
\begin{equation*}
  \begin{split}
    P_{\text{Lagrange}}(x)
    &= (50 - 64\sqrt{2} + 27\sqrt{3})\frac{x}{2\pi}
    - (217 - 352\sqrt{2} + 162\sqrt{3})\frac{x^{2}}{\pi^{2}}\\
    &\quad+ (33 - 64\sqrt{2} + 33\sqrt{3})\frac{18}{\pi^{3}}x^{3}
    - (7 - 16\sqrt{2} + 9\sqrt{3})\frac{72}{\pi^{4}}x^{4}
  \end{split}
\end{equation*}
which is clearly a different polynomial (they're of different orders!).

So which approximation is better? Well, it depends on what you mean by
``better''. But one measure of goodness is the $L^{2}$-distance between
the polynomial and $\sin(x)$ over the interval $0\leq x\leq \pi/2$. This
is a straightforward, though tedious, calculation:
\begin{equation}
  \begin{split}
  \int^{\pi/2}_{0}&(P(x)-\sin(x))^{2}\,\D x\\
 =& \frac{120960-276480\sqrt{2}+155520 \sqrt{3}}{\pi ^4}
  +\frac{-27648+65664\sqrt{2}-37584 \sqrt{3}}{\pi^3}\\
  &+\frac{-3864+8256 \sqrt{2}-4500\sqrt{3}}{\pi^2}
  +\frac{102-304 \sqrt{2}+189 \sqrt{3}}{\pi}\\
  &+7 - 16\sqrt{2} + 9\sqrt{3}
+\left(1294238-392832 \sqrt{2}+238788 \sqrt{3}-440352 \sqrt{6}\right)
  \frac{\pi }{147840}\\
&+\left(52459-13112 \sqrt{2}+8415 \sqrt{3}-19800 \sqrt{6}\right)
\frac{\pi^2}{147840}\\
&+\left(2010-560 \sqrt{2}+315 \sqrt{3}-720 \sqrt{6}\right) \frac{\pi^3}{147840}\\
\approx&4.029670120332435\times 10^{-5}
  \end{split}
\end{equation}
And for Lagrange interpolation:
\begin{equation}
  \begin{split}
  &\int^{\pi/2}_{0}(P_{\text{Lag}}(x)-\sin(x))^{2}\,\D x\\
  =& \frac{3456}{\pi ^4} \left(7-16 \sqrt{2}+9 \sqrt{3}\right)
  +\frac{216}{\pi ^3}\left(-23+64 \sqrt{2}-39 \sqrt{3}\right)\\
  &+\frac{4}{\pi ^2} \left(-217+352\sqrt{2}-162 \sqrt{3}\right)
  +\frac{\pi}{6720} \left(6948-1648 \sqrt{2}+1107 \sqrt{3}-1296 \sqrt{6}\right) \\
  &+\frac{-3-64 \sqrt{2}+54 \sqrt{3}}{\pi}\\
  \approx&3.20243293420500498\times 10^{-8}
  \end{split}
\end{equation}
So we find Newton's divided difference is slightly worse by this metric.
\end{ex}
